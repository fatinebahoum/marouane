[2024-01-20T19:45:15.414+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T19:45:15.419+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T19:45:15.420+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T19:45:15.431+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T19:45:15.435+0000] {standard_task_runner.py:57} INFO - Started process 45 to run task
[2024-01-20T19:45:15.438+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1194', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmp1xwdqfi8']
[2024-01-20T19:45:15.439+0000] {standard_task_runner.py:85} INFO - Job 1194: Subtask submit_spark_job
[2024-01-20T19:45:15.478+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T19:45:15.534+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T19:45:15.541+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T19:45:15.542+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T19:45:15.647+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T19:45:16.864+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T19:45:17.022+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T19:45:17.023+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T19:45:17.026+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T19:45:17.026+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-8d4e5c4c-fd2e-4e37-90ac-2f2bb8d015b2;1.0
[2024-01-20T19:45:17.027+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T19:45:17.101+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T19:45:17.126+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T19:45:17.148+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T19:45:17.181+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T19:45:17.203+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T19:45:17.219+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T19:45:17.233+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T19:45:17.247+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T19:45:17.257+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T19:45:17.271+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T19:45:17.279+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T19:45:17.289+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T19:45:17.297+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T19:45:17.309+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T19:45:17.321+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T19:45:17.334+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T19:45:17.357+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T19:45:17.365+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T19:45:17.371+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T19:45:17.849+0000] {spark_submit.py:571} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...
[2024-01-20T19:45:17.908+0000] {spark_submit.py:571} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (516ms)
[2024-01-20T19:45:17.913+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 359ms :: artifacts dl 527ms
[2024-01-20T19:45:17.914+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T19:45:17.914+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T19:45:17.915+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T19:45:17.915+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T19:45:17.916+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T19:45:17.916+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T19:45:17.917+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T19:45:17.917+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T19:45:17.917+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T19:45:17.918+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T19:45:17.918+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T19:45:17.919+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T19:45:17.919+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T19:45:17.919+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T19:45:17.920+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T19:45:17.920+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T19:45:17.921+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T19:45:17.921+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T19:45:17.922+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T19:45:17.922+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T19:45:17.922+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T19:45:17.923+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T19:45:17.923+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T19:45:17.924+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T19:45:17.924+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   1   |
[2024-01-20T19:45:17.925+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T19:45:17.925+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-8d4e5c4c-fd2e-4e37-90ac-2f2bb8d015b2
[2024-01-20T19:45:17.926+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T19:45:17.928+0000] {spark_submit.py:571} INFO - 1 artifacts copied, 18 already retrieved (40kB/11ms)
[2024-01-20T19:45:18.062+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T19:45:26.599+0000] {spark_submit.py:571} INFO - 2024-01-20 19:45:26.599108: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T19:45:26.657+0000] {spark_submit.py:571} INFO - 2024-01-20 19:45:26.657106: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T19:45:26.965+0000] {spark_submit.py:571} INFO - 2024-01-20 19:45:26.965753: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T19:45:26.966+0000] {spark_submit.py:571} INFO - 2024-01-20 19:45:26.965828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T19:45:27.023+0000] {spark_submit.py:571} INFO - 2024-01-20 19:45:27.023141: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T19:45:27.139+0000] {spark_submit.py:571} INFO - 2024-01-20 19:45:27.138873: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T19:45:27.140+0000] {spark_submit.py:571} INFO - 2024-01-20 19:45:27.140070: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T19:45:27.140+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T19:45:28.332+0000] {spark_submit.py:571} INFO - 2024-01-20 19:45:28.332339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T19:45:31.449+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T19:45:31.466+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO ResourceUtils: ==============================================================
[2024-01-20T19:45:31.467+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T19:45:31.467+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO ResourceUtils: ==============================================================
[2024-01-20T19:45:31.468+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T19:45:31.488+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T19:45:31.505+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T19:45:31.506+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T19:45:31.550+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T19:45:31.550+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T19:45:31.551+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T19:45:31.552+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T19:45:31.552+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T19:45:31.711+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO Utils: Successfully started service 'sparkDriver' on port 33715.
[2024-01-20T19:45:31.737+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T19:45:31.766+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T19:45:31.778+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T19:45:31.779+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T19:45:31.782+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T19:45:31.799+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1a723089-71e3-45ec-874e-a7b0c7142d81
[2024-01-20T19:45:31.811+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T19:45:31.828+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T19:45:31.945+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T19:45:31.988+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T19:45:32.013+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:33715/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.014+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:33715/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.014+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:33715/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.014+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:33715/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.015+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:33715/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.016+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:33715/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705779931442
[2024-01-20T19:45:32.016+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:33715/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705779931442
[2024-01-20T19:45:32.017+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:33715/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705779931442
[2024-01-20T19:45:32.017+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:33715/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.018+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:33715/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.018+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:33715/jars/com.typesafe_config-1.4.1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.019+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:33715/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705779931442
[2024-01-20T19:45:32.019+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:33715/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705779931442
[2024-01-20T19:45:32.019+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:33715/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705779931442
[2024-01-20T19:45:32.019+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:33715/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705779931442
[2024-01-20T19:45:32.020+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:33715/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.020+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:33715/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705779931442
[2024-01-20T19:45:32.020+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:33715/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705779931442
[2024-01-20T19:45:32.021+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:33715/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.021+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:33715/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.021+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T19:45:32.031+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:33715/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.032+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T19:45:32.035+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:33715/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.036+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T19:45:32.039+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:33715/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.040+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T19:45:32.080+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:33715/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.080+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T19:45:32.083+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:33715/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705779931442
[2024-01-20T19:45:32.084+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T19:45:32.088+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:33715/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705779931442
[2024-01-20T19:45:32.089+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T19:45:32.093+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:33715/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705779931442
[2024-01-20T19:45:32.094+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T19:45:32.111+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:33715/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.112+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T19:45:32.118+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:33715/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.118+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T19:45:32.132+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:33715/files/com.typesafe_config-1.4.1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.132+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.typesafe_config-1.4.1.jar
[2024-01-20T19:45:32.136+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:33715/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705779931442
[2024-01-20T19:45:32.136+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T19:45:32.140+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:33715/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705779931442
[2024-01-20T19:45:32.140+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T19:45:32.143+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:33715/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705779931442
[2024-01-20T19:45:32.144+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T19:45:32.148+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:33715/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705779931442
[2024-01-20T19:45:32.149+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T19:45:32.151+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:33715/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705779931442
[2024-01-20T19:45:32.152+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T19:45:32.154+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:33715/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705779931442
[2024-01-20T19:45:32.154+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T19:45:32.157+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:33715/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705779931442
[2024-01-20T19:45:32.157+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T19:45:32.161+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:33715/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705779931442
[2024-01-20T19:45:32.161+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-2e8b8aef-fb8c-4b42-b23e-f5f9440ff377/userFiles-257f8cd4-78c7-4a82-b7ef-c02b429925b4/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T19:45:32.283+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T19:45:32.326+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 22 ms (0 ms spent in bootstraps)
[2024-01-20T19:45:32.415+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120194532-0003
[2024-01-20T19:45:32.416+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120194532-0003/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T19:45:32.418+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120194532-0003/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T19:45:32.421+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42733.
[2024-01-20T19:45:32.422+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:42733
[2024-01-20T19:45:32.423+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T19:45:32.428+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 42733, None)
[2024-01-20T19:45:32.431+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:42733 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 42733, None)
[2024-01-20T19:45:32.433+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 42733, None)
[2024-01-20T19:45:32.434+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 42733, None)
[2024-01-20T19:45:32.475+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120194532-0003/0 is now RUNNING
[2024-01-20T19:45:32.607+0000] {spark_submit.py:571} INFO - 24/01/20 19:45:32 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T19:45:32.833+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T19:45:32.837+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T19:45:32.838+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T19:45:32.870+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T19:45:32.872+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T19:45:32.872+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T19:45:32.877+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T19:45:32.878+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 198, in <module>
[2024-01-20T19:45:32.878+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T19:45:32.878+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 116, in batch_processing
[2024-01-20T19:45:32.879+0000] {spark_submit.py:571} INFO - first_timestamp_result = session.execute(query_first_timestamp).one()
[2024-01-20T19:45:32.879+0000] {spark_submit.py:571} INFO - File "cassandra/cluster.py", line 2637, in cassandra.cluster.Session.execute
[2024-01-20T19:45:32.879+0000] {spark_submit.py:571} INFO - File "cassandra/cluster.py", line 4920, in cassandra.cluster.ResponseFuture.result
[2024-01-20T19:45:32.880+0000] {spark_submit.py:571} INFO - cassandra.InvalidRequest: Error from server: code=2200 [Invalid query] message="No keyspace has been specified. USE a keyspace, or explicitly specify keyspace.tablename"
[2024-01-20T19:45:33.628+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T19:45:33.632+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T194515, end_date=20240120T194533
[2024-01-20T19:45:33.647+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1194 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 45)
[2024-01-20T19:45:33.659+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T19:45:33.670+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T19:53:10.917+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T19:53:10.922+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T19:53:10.922+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T19:53:10.933+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T19:53:10.936+0000] {standard_task_runner.py:57} INFO - Started process 49 to run task
[2024-01-20T19:53:10.938+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1206', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmptoqwt9v2']
[2024-01-20T19:53:10.939+0000] {standard_task_runner.py:85} INFO - Job 1206: Subtask submit_spark_job
[2024-01-20T19:53:10.976+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T19:53:11.032+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T19:53:11.039+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T19:53:11.040+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T19:53:11.144+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T19:53:12.519+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T19:53:12.643+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T19:53:12.643+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T19:53:12.646+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T19:53:12.647+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-4c51c9ae-56ae-4937-9132-a679c563f7ef;1.0
[2024-01-20T19:53:12.647+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T19:53:12.716+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T19:53:12.737+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T19:53:12.755+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T19:53:12.776+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T19:53:12.792+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T19:53:12.805+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T19:53:12.821+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T19:53:12.832+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T19:53:12.842+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T19:53:12.854+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T19:53:12.866+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T19:53:12.878+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T19:53:12.886+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T19:53:12.894+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T19:53:12.904+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T19:53:12.914+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T19:53:12.942+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T19:53:12.950+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T19:53:12.957+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T19:53:12.985+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 324ms :: artifacts dl 15ms
[2024-01-20T19:53:12.986+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T19:53:12.986+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T19:53:12.986+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T19:53:12.987+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T19:53:12.987+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T19:53:12.987+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T19:53:12.988+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T19:53:12.988+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T19:53:12.989+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T19:53:12.989+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T19:53:12.989+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T19:53:12.990+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T19:53:12.990+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T19:53:12.991+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T19:53:12.991+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T19:53:12.991+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T19:53:12.992+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T19:53:12.992+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T19:53:12.992+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T19:53:12.993+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T19:53:12.993+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T19:53:12.994+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T19:53:12.994+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T19:53:12.994+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T19:53:12.995+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T19:53:12.995+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T19:53:12.995+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-4c51c9ae-56ae-4937-9132-a679c563f7ef
[2024-01-20T19:53:12.996+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T19:53:13.003+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/9ms)
[2024-01-20T19:53:13.127+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T19:53:21.610+0000] {spark_submit.py:571} INFO - 2024-01-20 19:53:21.609726: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T19:53:21.613+0000] {spark_submit.py:571} INFO - 2024-01-20 19:53:21.613294: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T19:53:21.640+0000] {spark_submit.py:571} INFO - 2024-01-20 19:53:21.640319: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T19:53:21.641+0000] {spark_submit.py:571} INFO - 2024-01-20 19:53:21.640369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T19:53:21.642+0000] {spark_submit.py:571} INFO - 2024-01-20 19:53:21.642485: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T19:53:21.648+0000] {spark_submit.py:571} INFO - 2024-01-20 19:53:21.648347: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T19:53:21.648+0000] {spark_submit.py:571} INFO - 2024-01-20 19:53:21.648545: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T19:53:21.649+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T19:53:22.716+0000] {spark_submit.py:571} INFO - 2024-01-20 19:53:22.716317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T19:53:25.307+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T19:53:25.326+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO ResourceUtils: ==============================================================
[2024-01-20T19:53:25.326+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T19:53:25.327+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO ResourceUtils: ==============================================================
[2024-01-20T19:53:25.327+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T19:53:25.344+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T19:53:25.355+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T19:53:25.356+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T19:53:25.413+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T19:53:25.414+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T19:53:25.414+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T19:53:25.415+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T19:53:25.415+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T19:53:25.587+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Successfully started service 'sparkDriver' on port 37625.
[2024-01-20T19:53:25.622+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T19:53:25.648+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T19:53:25.663+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T19:53:25.663+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T19:53:25.666+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T19:53:25.687+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6e4b9e0a-5d5d-4886-b4ae-80c5671029d7
[2024-01-20T19:53:25.703+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T19:53:25.719+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T19:53:25.836+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T19:53:25.884+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T19:53:25.911+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:37625/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.912+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:37625/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.912+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:37625/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.912+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:37625/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.913+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:37625/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.913+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:37625/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705780405301
[2024-01-20T19:53:25.913+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:37625/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705780405301
[2024-01-20T19:53:25.913+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:37625/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705780405301
[2024-01-20T19:53:25.914+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:37625/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.914+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:37625/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.914+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:37625/jars/com.typesafe_config-1.4.1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.914+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:37625/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705780405301
[2024-01-20T19:53:25.915+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:37625/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705780405301
[2024-01-20T19:53:25.915+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:37625/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705780405301
[2024-01-20T19:53:25.915+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:37625/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705780405301
[2024-01-20T19:53:25.916+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:37625/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.916+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:37625/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705780405301
[2024-01-20T19:53:25.916+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:37625/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705780405301
[2024-01-20T19:53:25.916+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:37625/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.917+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:37625/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.917+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T19:53:25.929+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:37625/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.930+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T19:53:25.935+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:37625/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.936+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T19:53:25.939+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:37625/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.939+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T19:53:25.956+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:37625/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.956+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T19:53:25.958+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:37625/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705780405301
[2024-01-20T19:53:25.959+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T19:53:25.962+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:37625/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705780405301
[2024-01-20T19:53:25.962+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T19:53:25.964+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:37625/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705780405301
[2024-01-20T19:53:25.965+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T19:53:25.975+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:37625/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705780405301
[2024-01-20T19:53:25.976+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T19:53:25.979+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:37625/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.979+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T19:53:25.988+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:37625/files/com.typesafe_config-1.4.1.jar with timestamp 1705780405301
[2024-01-20T19:53:25.988+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.typesafe_config-1.4.1.jar
[2024-01-20T19:53:25.991+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:37625/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705780405301
[2024-01-20T19:53:25.991+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T19:53:25.994+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:37625/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705780405301
[2024-01-20T19:53:25.995+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T19:53:25.998+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:37625/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705780405301
[2024-01-20T19:53:25.999+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:25 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T19:53:26.002+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:37625/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705780405301
[2024-01-20T19:53:26.003+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T19:53:26.005+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:37625/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705780405301
[2024-01-20T19:53:26.005+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T19:53:26.007+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:37625/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705780405301
[2024-01-20T19:53:26.008+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T19:53:26.010+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:37625/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705780405301
[2024-01-20T19:53:26.011+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T19:53:26.013+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:37625/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705780405301
[2024-01-20T19:53:26.014+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-e4f99b6c-3895-4588-9f5d-b247db92d6b3/userFiles-89ffbcf2-00cf-4f87-a2fe-267c37672e3e/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T19:53:26.076+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T19:53:26.103+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 16 ms (0 ms spent in bootstraps)
[2024-01-20T19:53:26.172+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120195326-0006
[2024-01-20T19:53:26.173+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120195326-0006/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T19:53:26.175+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120195326-0006/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T19:53:26.179+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35287.
[2024-01-20T19:53:26.180+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:35287
[2024-01-20T19:53:26.181+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T19:53:26.187+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 35287, None)
[2024-01-20T19:53:26.190+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:35287 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 35287, None)
[2024-01-20T19:53:26.192+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 35287, None)
[2024-01-20T19:53:26.192+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 35287, None)
[2024-01-20T19:53:26.223+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120195326-0006/0 is now RUNNING
[2024-01-20T19:53:26.382+0000] {spark_submit.py:571} INFO - 24/01/20 19:53:26 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T19:53:26.545+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T19:53:26.550+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T19:53:26.551+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T19:53:26.573+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T19:53:27.696+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T19:53:27.697+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T19:53:27.796+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T19:53:27.797+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 198, in <module>
[2024-01-20T19:53:27.797+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T19:53:27.797+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 128, in batch_processing
[2024-01-20T19:53:27.798+0000] {spark_submit.py:571} INFO - change_from_kafka = ((close_price - open_price) / open_price) * 100
[2024-01-20T19:53:27.798+0000] {spark_submit.py:571} INFO - ZeroDivisionError: float division by zero
[2024-01-20T19:53:28.571+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T19:53:28.574+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T195310, end_date=20240120T195328
[2024-01-20T19:53:28.587+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1206 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 49)
[2024-01-20T19:53:28.615+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T19:53:28.627+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T20:00:52.253+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T20:00:52.259+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T20:00:52.259+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T20:00:52.271+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T20:00:52.274+0000] {standard_task_runner.py:57} INFO - Started process 37 to run task
[2024-01-20T20:00:52.276+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1227', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmpd0nw6zg_']
[2024-01-20T20:00:52.278+0000] {standard_task_runner.py:85} INFO - Job 1227: Subtask submit_spark_job
[2024-01-20T20:00:52.316+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T20:00:52.381+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T20:00:52.390+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T20:00:52.391+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T20:00:52.530+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T20:00:53.739+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T20:00:53.859+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T20:00:53.860+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T20:00:53.862+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T20:00:53.863+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-08e5e3cc-4d1a-4db9-9aee-2a92a17387bf;1.0
[2024-01-20T20:00:53.864+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T20:00:53.931+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T20:00:53.951+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T20:00:53.969+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T20:00:53.992+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T20:00:54.008+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T20:00:54.021+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T20:00:54.035+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T20:00:54.047+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T20:00:54.058+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T20:00:54.074+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T20:00:54.085+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T20:00:54.097+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T20:00:54.106+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T20:00:54.118+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T20:00:54.131+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T20:00:54.148+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T20:00:54.168+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T20:00:54.200+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T20:00:54.214+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T20:00:54.260+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 374ms :: artifacts dl 23ms
[2024-01-20T20:00:54.261+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T20:00:54.261+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T20:00:54.262+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T20:00:54.263+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T20:00:54.263+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T20:00:54.264+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T20:00:54.264+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T20:00:54.265+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T20:00:54.266+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T20:00:54.266+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T20:00:54.267+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T20:00:54.267+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T20:00:54.268+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T20:00:54.268+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T20:00:54.269+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T20:00:54.269+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T20:00:54.270+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T20:00:54.270+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T20:00:54.271+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T20:00:54.272+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T20:00:54.273+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:00:54.273+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T20:00:54.274+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T20:00:54.275+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:00:54.275+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T20:00:54.276+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:00:54.276+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-08e5e3cc-4d1a-4db9-9aee-2a92a17387bf
[2024-01-20T20:00:54.277+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T20:00:54.289+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/15ms)
[2024-01-20T20:00:54.462+0000] {spark_submit.py:571} INFO - 24/01/20 20:00:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T20:01:03.184+0000] {spark_submit.py:571} INFO - 2024-01-20 20:01:03.184679: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T20:01:03.188+0000] {spark_submit.py:571} INFO - 2024-01-20 20:01:03.188678: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T20:01:03.219+0000] {spark_submit.py:571} INFO - 2024-01-20 20:01:03.219411: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T20:01:03.220+0000] {spark_submit.py:571} INFO - 2024-01-20 20:01:03.219466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T20:01:03.220+0000] {spark_submit.py:571} INFO - 2024-01-20 20:01:03.220829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T20:01:03.226+0000] {spark_submit.py:571} INFO - 2024-01-20 20:01:03.226503: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T20:01:03.227+0000] {spark_submit.py:571} INFO - 2024-01-20 20:01:03.226710: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T20:01:03.227+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T20:01:04.320+0000] {spark_submit.py:571} INFO - 2024-01-20 20:01:04.320278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T20:01:07.008+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T20:01:07.026+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO ResourceUtils: ==============================================================
[2024-01-20T20:01:07.027+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T20:01:07.027+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO ResourceUtils: ==============================================================
[2024-01-20T20:01:07.028+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T20:01:07.044+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T20:01:07.055+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T20:01:07.056+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T20:01:07.106+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T20:01:07.107+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T20:01:07.107+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T20:01:07.108+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T20:01:07.108+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T20:01:07.266+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Successfully started service 'sparkDriver' on port 37813.
[2024-01-20T20:01:07.284+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T20:01:07.307+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T20:01:07.316+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T20:01:07.317+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T20:01:07.319+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T20:01:07.333+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-73a6631d-b4a6-43e3-a7ae-414c1d435743
[2024-01-20T20:01:07.343+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T20:01:07.352+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T20:01:07.454+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T20:01:07.496+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T20:01:07.523+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:37813/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.524+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:37813/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.524+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:37813/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.525+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:37813/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.525+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:37813/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.525+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:37813/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705780867001
[2024-01-20T20:01:07.526+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:37813/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705780867001
[2024-01-20T20:01:07.526+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:37813/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705780867001
[2024-01-20T20:01:07.526+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:37813/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.527+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:37813/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.527+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:37813/jars/com.typesafe_config-1.4.1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.528+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:37813/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705780867001
[2024-01-20T20:01:07.528+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:37813/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705780867001
[2024-01-20T20:01:07.528+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:37813/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705780867001
[2024-01-20T20:01:07.529+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:37813/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705780867001
[2024-01-20T20:01:07.529+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:37813/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.529+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:37813/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705780867001
[2024-01-20T20:01:07.530+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:37813/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705780867001
[2024-01-20T20:01:07.530+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:37813/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.531+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:37813/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.531+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T20:01:07.537+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:37813/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.538+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T20:01:07.542+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:37813/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.543+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T20:01:07.546+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:37813/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.546+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T20:01:07.561+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:37813/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.562+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T20:01:07.565+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:37813/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705780867001
[2024-01-20T20:01:07.565+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T20:01:07.569+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:37813/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705780867001
[2024-01-20T20:01:07.570+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T20:01:07.573+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:37813/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705780867001
[2024-01-20T20:01:07.573+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T20:01:07.585+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:37813/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.585+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T20:01:07.589+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:37813/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.590+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T20:01:07.595+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:37813/files/com.typesafe_config-1.4.1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.595+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.typesafe_config-1.4.1.jar
[2024-01-20T20:01:07.599+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:37813/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705780867001
[2024-01-20T20:01:07.599+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T20:01:07.603+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:37813/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705780867001
[2024-01-20T20:01:07.603+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T20:01:07.606+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:37813/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705780867001
[2024-01-20T20:01:07.607+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T20:01:07.610+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:37813/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705780867001
[2024-01-20T20:01:07.610+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T20:01:07.615+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:37813/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705780867001
[2024-01-20T20:01:07.616+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T20:01:07.622+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:37813/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705780867001
[2024-01-20T20:01:07.623+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T20:01:07.626+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:37813/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705780867001
[2024-01-20T20:01:07.627+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T20:01:07.630+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:37813/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705780867001
[2024-01-20T20:01:07.631+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-c667db5f-21cd-4904-a83c-1341e4c07a66/userFiles-b2461734-e3c7-4041-9fb2-5e76b5d35ef5/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T20:01:07.695+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T20:01:07.724+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 17 ms (0 ms spent in bootstraps)
[2024-01-20T20:01:07.789+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120200107-0009
[2024-01-20T20:01:07.791+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120200107-0009/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T20:01:07.793+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120200107-0009/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T20:01:07.794+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37149.
[2024-01-20T20:01:07.795+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:37149
[2024-01-20T20:01:07.796+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T20:01:07.801+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 37149, None)
[2024-01-20T20:01:07.805+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:37149 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 37149, None)
[2024-01-20T20:01:07.807+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 37149, None)
[2024-01-20T20:01:07.808+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 37149, None)
[2024-01-20T20:01:07.828+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120200107-0009/0 is now RUNNING
[2024-01-20T20:01:07.960+0000] {spark_submit.py:571} INFO - 24/01/20 20:01:07 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T20:01:08.148+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T20:01:08.152+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T20:01:08.154+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T20:01:08.175+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T20:01:08.177+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T20:01:08.177+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T20:01:08.210+0000] {spark_submit.py:571} INFO - model_path :  /opt/***/shared_volume/best_model_sen.h5
[2024-01-20T20:01:08.881+0000] {spark_submit.py:571} INFO - [[0.]]
[2024-01-20T20:01:08.883+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T20:01:08.883+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 201, in <module>
[2024-01-20T20:01:08.883+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T20:01:08.884+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 168, in batch_processing
[2024-01-20T20:01:08.884+0000] {spark_submit.py:571} INFO - predicted_bitcoin_price = predict_next_day(prices_array1, loaded_model, scaler)
[2024-01-20T20:01:08.885+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 24, in predict_next_day
[2024-01-20T20:01:08.885+0000] {spark_submit.py:571} INFO - last_days = data.reshape(1, time_step, 1)
[2024-01-20T20:01:08.885+0000] {spark_submit.py:571} INFO - ValueError: cannot reshape array of size 1 into shape (1,5,1)
[2024-01-20T20:01:09.701+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T20:01:09.704+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T200052, end_date=20240120T200109
[2024-01-20T20:01:09.717+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1227 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 37)
[2024-01-20T20:01:09.731+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T20:01:09.743+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T20:10:43.747+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T20:10:43.754+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T20:10:43.754+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T20:10:43.768+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T20:10:43.771+0000] {standard_task_runner.py:57} INFO - Started process 52 to run task
[2024-01-20T20:10:43.773+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1240', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmpm9ptk69h']
[2024-01-20T20:10:43.775+0000] {standard_task_runner.py:85} INFO - Job 1240: Subtask submit_spark_job
[2024-01-20T20:10:43.821+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T20:10:43.894+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T20:10:43.902+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T20:10:43.904+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T20:10:44.017+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T20:10:45.434+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T20:10:45.602+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T20:10:45.603+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T20:10:45.606+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T20:10:45.607+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-70448879-74e9-48dc-a6b4-f78707c05802;1.0
[2024-01-20T20:10:45.607+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T20:10:45.672+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T20:10:45.691+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T20:10:45.712+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T20:10:45.735+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T20:10:45.754+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T20:10:45.772+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T20:10:45.789+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T20:10:45.800+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T20:10:45.853+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T20:10:45.863+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T20:10:45.875+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T20:10:45.889+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T20:10:45.899+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T20:10:45.911+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T20:10:45.923+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T20:10:45.934+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T20:10:45.958+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T20:10:45.966+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T20:10:45.973+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T20:10:46.004+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 384ms :: artifacts dl 14ms
[2024-01-20T20:10:46.005+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T20:10:46.005+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T20:10:46.006+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T20:10:46.006+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T20:10:46.007+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T20:10:46.007+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T20:10:46.008+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T20:10:46.008+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T20:10:46.008+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T20:10:46.009+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T20:10:46.009+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T20:10:46.009+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T20:10:46.010+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T20:10:46.010+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T20:10:46.011+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T20:10:46.011+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T20:10:46.011+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T20:10:46.012+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T20:10:46.012+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T20:10:46.013+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T20:10:46.013+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:10:46.013+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T20:10:46.014+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T20:10:46.014+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:10:46.015+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T20:10:46.015+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:10:46.015+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-70448879-74e9-48dc-a6b4-f78707c05802
[2024-01-20T20:10:46.016+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T20:10:46.022+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/10ms)
[2024-01-20T20:10:46.157+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T20:10:54.624+0000] {spark_submit.py:571} INFO - 2024-01-20 20:10:54.623861: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T20:10:54.626+0000] {spark_submit.py:571} INFO - 2024-01-20 20:10:54.626367: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T20:10:54.658+0000] {spark_submit.py:571} INFO - 2024-01-20 20:10:54.658663: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T20:10:54.659+0000] {spark_submit.py:571} INFO - 2024-01-20 20:10:54.658711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T20:10:54.660+0000] {spark_submit.py:571} INFO - 2024-01-20 20:10:54.660537: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T20:10:54.667+0000] {spark_submit.py:571} INFO - 2024-01-20 20:10:54.667584: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T20:10:54.670+0000] {spark_submit.py:571} INFO - 2024-01-20 20:10:54.667795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T20:10:54.719+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T20:10:55.825+0000] {spark_submit.py:571} INFO - 2024-01-20 20:10:55.825714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T20:10:58.313+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T20:10:58.331+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO ResourceUtils: ==============================================================
[2024-01-20T20:10:58.332+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T20:10:58.333+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO ResourceUtils: ==============================================================
[2024-01-20T20:10:58.333+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T20:10:58.348+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T20:10:58.361+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T20:10:58.362+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T20:10:58.406+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T20:10:58.406+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T20:10:58.407+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T20:10:58.407+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T20:10:58.408+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T20:10:58.598+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Successfully started service 'sparkDriver' on port 37137.
[2024-01-20T20:10:58.623+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T20:10:58.650+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T20:10:58.664+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T20:10:58.665+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T20:10:58.668+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T20:10:58.688+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e9cd0457-bb83-4db3-9887-b4838db41af9
[2024-01-20T20:10:58.702+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T20:10:58.715+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T20:10:58.818+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T20:10:58.873+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T20:10:58.901+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:37137/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.902+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:37137/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.903+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:37137/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.904+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:37137/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.905+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:37137/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.905+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:37137/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705781458307
[2024-01-20T20:10:58.905+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:37137/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705781458307
[2024-01-20T20:10:58.906+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:37137/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705781458307
[2024-01-20T20:10:58.906+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:37137/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.907+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:37137/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.907+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:37137/jars/com.typesafe_config-1.4.1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.907+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:37137/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705781458307
[2024-01-20T20:10:58.907+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:37137/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705781458307
[2024-01-20T20:10:58.908+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:37137/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705781458307
[2024-01-20T20:10:58.908+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:37137/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705781458307
[2024-01-20T20:10:58.908+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:37137/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.909+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:37137/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705781458307
[2024-01-20T20:10:58.910+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:37137/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705781458307
[2024-01-20T20:10:58.910+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:37137/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.910+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:37137/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.911+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T20:10:58.918+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:37137/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.918+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T20:10:58.926+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:37137/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.927+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T20:10:58.932+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:37137/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.933+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T20:10:58.945+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:37137/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.946+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T20:10:58.948+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:37137/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705781458307
[2024-01-20T20:10:58.949+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T20:10:58.952+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:37137/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705781458307
[2024-01-20T20:10:58.953+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T20:10:58.955+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:37137/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705781458307
[2024-01-20T20:10:58.955+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T20:10:58.965+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:37137/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705781458307
[2024-01-20T20:10:58.966+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T20:10:58.968+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:37137/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.969+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T20:10:58.974+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:37137/files/com.typesafe_config-1.4.1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.975+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.typesafe_config-1.4.1.jar
[2024-01-20T20:10:58.978+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:37137/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705781458307
[2024-01-20T20:10:58.979+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T20:10:58.982+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:37137/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705781458307
[2024-01-20T20:10:58.983+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T20:10:58.985+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:37137/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705781458307
[2024-01-20T20:10:58.986+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T20:10:58.989+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:37137/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705781458307
[2024-01-20T20:10:58.989+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T20:10:58.992+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:37137/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705781458307
[2024-01-20T20:10:58.992+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T20:10:58.995+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:37137/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705781458307
[2024-01-20T20:10:58.996+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T20:10:59.000+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:37137/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705781458307
[2024-01-20T20:10:59.001+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:58 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T20:10:59.002+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:37137/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705781458307
[2024-01-20T20:10:59.004+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-c12493e8-f93a-471d-90ba-2f816a133073/userFiles-bd0b4ba1-9943-4201-99f5-e13296e9f8c5/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T20:10:59.058+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T20:10:59.085+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 16 ms (0 ms spent in bootstraps)
[2024-01-20T20:10:59.152+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120201059-0013
[2024-01-20T20:10:59.154+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120201059-0013/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T20:10:59.156+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120201059-0013/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T20:10:59.158+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34583.
[2024-01-20T20:10:59.158+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:34583
[2024-01-20T20:10:59.159+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T20:10:59.164+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 34583, None)
[2024-01-20T20:10:59.167+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:34583 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 34583, None)
[2024-01-20T20:10:59.168+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 34583, None)
[2024-01-20T20:10:59.169+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 34583, None)
[2024-01-20T20:10:59.189+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120201059-0013/0 is now RUNNING
[2024-01-20T20:10:59.328+0000] {spark_submit.py:571} INFO - 24/01/20 20:10:59 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T20:10:59.511+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T20:10:59.515+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T20:10:59.516+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T20:10:59.541+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T20:10:59.543+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T20:10:59.544+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T20:10:59.544+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes
[2024-01-20T20:10:59.549+0000] {spark_submit.py:571} INFO - first_timestamp_result : None
[2024-01-20T20:10:59.553+0000] {spark_submit.py:571} INFO - last_timestamp_result : Row(bitcoin_price=41664.289302)
[2024-01-20T20:10:59.557+0000] {spark_submit.py:571} INFO - sum_posts_result : Row(system_sum_posts_last_5_minutes=0)
[2024-01-20T20:10:59.558+0000] {spark_submit.py:571} INFO - Obtenez les valeurs n√©cessaires
[2024-01-20T20:10:59.558+0000] {spark_submit.py:571} INFO - first_timestamp_bitcoin_price :  0.0
[2024-01-20T20:10:59.559+0000] {spark_submit.py:571} INFO - last_timestamp_bitcoin_price :  41664.289302
[2024-01-20T20:10:59.559+0000] {spark_submit.py:571} INFO - sum_posts_last_day :  0
[2024-01-20T20:10:59.560+0000] {spark_submit.py:571} INFO - change_from_kafka : 0
[2024-01-20T20:10:59.560+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes ...
[2024-01-20T20:10:59.561+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish_result :  Row(system_avg_avg_sentiment_bullish=0.0)
[2024-01-20T20:10:59.565+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish_result :  Row(system_avg_avg_sentiment_bearish=0.0)
[2024-01-20T20:10:59.568+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral_result :  Row(system_avg_avg_sentiment_neutral=0.0)
[2024-01-20T20:10:59.569+0000] {spark_submit.py:571} INFO - # Obtenez les valeurs moyennes n√©cessaires
[2024-01-20T20:10:59.569+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish :  0.0
[2024-01-20T20:10:59.570+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish :  0.0
[2024-01-20T20:10:59.570+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral :  0.0
[2024-01-20T20:10:59.570+0000] {spark_submit.py:571} INFO - # Ex√©cutez les requ√™tes...
[2024-01-20T20:10:59.576+0000] {spark_submit.py:571} INFO - max_price :  None
[2024-01-20T20:10:59.577+0000] {spark_submit.py:571} INFO - min_price :  None
[2024-01-20T20:10:59.578+0000] {spark_submit.py:571} INFO - model_path :  /opt/***/shared_volume/best_model_sen.h5
[2024-01-20T20:11:00.249+0000] {spark_submit.py:571} INFO - [[0.]]
[2024-01-20T20:11:00.250+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T20:11:00.250+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 220, in <module>
[2024-01-20T20:11:00.251+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T20:11:00.251+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 187, in batch_processing
[2024-01-20T20:11:00.251+0000] {spark_submit.py:571} INFO - predicted_bitcoin_price = predict_next_day(prices_array1, loaded_model, scaler)
[2024-01-20T20:11:00.252+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 24, in predict_next_day
[2024-01-20T20:11:00.252+0000] {spark_submit.py:571} INFO - last_days = data.reshape(1, time_step, 1)
[2024-01-20T20:11:00.253+0000] {spark_submit.py:571} INFO - ValueError: cannot reshape array of size 1 into shape (1,5,1)
[2024-01-20T20:11:01.019+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T20:11:01.023+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T201043, end_date=20240120T201101
[2024-01-20T20:11:01.036+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1240 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 52)
[2024-01-20T20:11:01.059+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T20:11:01.071+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T20:28:35.405+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T20:28:35.411+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T20:28:35.412+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T20:28:35.423+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T20:28:35.427+0000] {standard_task_runner.py:57} INFO - Started process 31 to run task
[2024-01-20T20:28:35.429+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1258', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmpw0hamjur']
[2024-01-20T20:28:35.431+0000] {standard_task_runner.py:85} INFO - Job 1258: Subtask submit_spark_job
[2024-01-20T20:28:35.471+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T20:28:35.532+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T20:28:35.540+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T20:28:35.541+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T20:28:35.655+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T20:28:37.064+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T20:28:37.185+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T20:28:37.185+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T20:28:37.188+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T20:28:37.189+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-66ba947d-10dd-4bae-be82-68189033005c;1.0
[2024-01-20T20:28:37.189+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T20:28:37.280+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T20:28:37.308+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T20:28:37.334+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T20:28:37.362+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T20:28:37.386+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T20:28:37.402+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T20:28:37.416+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T20:28:37.434+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T20:28:37.450+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T20:28:37.467+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T20:28:37.481+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T20:28:37.492+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T20:28:37.505+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T20:28:37.517+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T20:28:37.531+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T20:28:37.545+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T20:28:37.560+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T20:28:37.568+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T20:28:37.577+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T20:28:37.637+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 428ms :: artifacts dl 21ms
[2024-01-20T20:28:37.638+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T20:28:37.639+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T20:28:37.639+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T20:28:37.640+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T20:28:37.641+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T20:28:37.641+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T20:28:37.643+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T20:28:37.643+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T20:28:37.644+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T20:28:37.644+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T20:28:37.645+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T20:28:37.645+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T20:28:37.646+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T20:28:37.647+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T20:28:37.647+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T20:28:37.648+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T20:28:37.648+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T20:28:37.648+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T20:28:37.649+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T20:28:37.649+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T20:28:37.649+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:28:37.650+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T20:28:37.650+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T20:28:37.651+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:28:37.651+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T20:28:37.652+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:28:37.653+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-66ba947d-10dd-4bae-be82-68189033005c
[2024-01-20T20:28:37.653+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T20:28:37.658+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/12ms)
[2024-01-20T20:28:37.819+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T20:28:46.867+0000] {spark_submit.py:571} INFO - 2024-01-20 20:28:46.866885: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T20:28:46.869+0000] {spark_submit.py:571} INFO - 2024-01-20 20:28:46.869178: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T20:28:46.901+0000] {spark_submit.py:571} INFO - 2024-01-20 20:28:46.901469: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T20:28:46.902+0000] {spark_submit.py:571} INFO - 2024-01-20 20:28:46.901531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T20:28:46.903+0000] {spark_submit.py:571} INFO - 2024-01-20 20:28:46.902982: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T20:28:46.909+0000] {spark_submit.py:571} INFO - 2024-01-20 20:28:46.909258: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T20:28:46.910+0000] {spark_submit.py:571} INFO - 2024-01-20 20:28:46.909512: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T20:28:46.911+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T20:28:48.105+0000] {spark_submit.py:571} INFO - 2024-01-20 20:28:48.104880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T20:28:51.155+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T20:28:51.171+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO ResourceUtils: ==============================================================
[2024-01-20T20:28:51.172+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T20:28:51.172+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO ResourceUtils: ==============================================================
[2024-01-20T20:28:51.172+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T20:28:51.190+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T20:28:51.201+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T20:28:51.201+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T20:28:51.240+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T20:28:51.241+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T20:28:51.242+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T20:28:51.243+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T20:28:51.243+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T20:28:51.422+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Successfully started service 'sparkDriver' on port 35095.
[2024-01-20T20:28:51.441+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T20:28:51.465+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T20:28:51.479+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T20:28:51.480+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T20:28:51.482+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T20:28:51.496+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3a6b1598-e9b0-492f-a3d7-afdd0e7da582
[2024-01-20T20:28:51.507+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T20:28:51.517+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T20:28:51.625+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T20:28:51.670+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T20:28:51.696+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:35095/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.697+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:35095/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.697+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:35095/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.698+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:35095/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.699+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:35095/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.699+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:35095/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705782531148
[2024-01-20T20:28:51.700+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:35095/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705782531148
[2024-01-20T20:28:51.700+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:35095/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705782531148
[2024-01-20T20:28:51.700+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:35095/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.701+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:35095/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.701+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:35095/jars/com.typesafe_config-1.4.1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.701+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:35095/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705782531148
[2024-01-20T20:28:51.701+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:35095/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705782531148
[2024-01-20T20:28:51.702+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:35095/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705782531148
[2024-01-20T20:28:51.702+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:35095/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705782531148
[2024-01-20T20:28:51.703+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:35095/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.703+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:35095/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705782531148
[2024-01-20T20:28:51.703+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:35095/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705782531148
[2024-01-20T20:28:51.704+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:35095/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.704+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:35095/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.704+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T20:28:51.712+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:35095/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.713+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T20:28:51.716+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:35095/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.717+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T20:28:51.720+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:35095/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.720+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T20:28:51.730+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:35095/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.730+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T20:28:51.733+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:35095/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705782531148
[2024-01-20T20:28:51.733+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T20:28:51.736+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:35095/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705782531148
[2024-01-20T20:28:51.736+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T20:28:51.739+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:35095/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705782531148
[2024-01-20T20:28:51.739+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T20:28:51.750+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:35095/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.751+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T20:28:51.754+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:35095/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.754+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T20:28:51.759+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:35095/files/com.typesafe_config-1.4.1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.759+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.typesafe_config-1.4.1.jar
[2024-01-20T20:28:51.762+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:35095/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705782531148
[2024-01-20T20:28:51.762+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T20:28:51.765+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:35095/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705782531148
[2024-01-20T20:28:51.766+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T20:28:51.768+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:35095/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705782531148
[2024-01-20T20:28:51.768+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T20:28:51.771+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:35095/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705782531148
[2024-01-20T20:28:51.772+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T20:28:51.774+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:35095/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705782531148
[2024-01-20T20:28:51.774+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T20:28:51.778+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:35095/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705782531148
[2024-01-20T20:28:51.778+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T20:28:51.782+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:35095/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705782531148
[2024-01-20T20:28:51.782+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T20:28:51.787+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:35095/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705782531148
[2024-01-20T20:28:51.788+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-97d5da6d-9df1-4d46-aaa5-46c7a0400b34/userFiles-6d792247-9011-4abb-acb6-cc1101273034/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T20:28:51.846+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T20:28:51.877+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 19 ms (0 ms spent in bootstraps)
[2024-01-20T20:28:51.948+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120202851-0016
[2024-01-20T20:28:51.949+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120202851-0016/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T20:28:51.951+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120202851-0016/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T20:28:51.952+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45467.
[2024-01-20T20:28:51.952+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:45467
[2024-01-20T20:28:51.954+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T20:28:51.957+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 45467, None)
[2024-01-20T20:28:51.960+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:45467 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 45467, None)
[2024-01-20T20:28:51.962+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 45467, None)
[2024-01-20T20:28:51.963+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 45467, None)
[2024-01-20T20:28:52.002+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:52 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120202851-0016/0 is now RUNNING
[2024-01-20T20:28:52.177+0000] {spark_submit.py:571} INFO - 24/01/20 20:28:52 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T20:28:52.485+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T20:28:52.497+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T20:28:52.503+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T20:28:52.552+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T20:28:52.554+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T20:28:52.554+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T20:28:52.555+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes
[2024-01-20T20:28:52.565+0000] {spark_submit.py:571} INFO - first_timestamp_result : None
[2024-01-20T20:28:52.570+0000] {spark_submit.py:571} INFO - last_timestamp_result : Row(bitcoin_price=41800.5678)
[2024-01-20T20:28:52.580+0000] {spark_submit.py:571} INFO - sum_posts_result : Row(system_sum_posts_last_5_minutes=0)
[2024-01-20T20:28:52.582+0000] {spark_submit.py:571} INFO - Obtenez les valeurs n√©cessaires
[2024-01-20T20:28:52.582+0000] {spark_submit.py:571} INFO - first_timestamp_bitcoin_price :  0.0
[2024-01-20T20:28:52.583+0000] {spark_submit.py:571} INFO - last_timestamp_bitcoin_price :  41800.5678
[2024-01-20T20:28:52.583+0000] {spark_submit.py:571} INFO - sum_posts_last_day :  0
[2024-01-20T20:28:52.584+0000] {spark_submit.py:571} INFO - change_from_kafka : 0
[2024-01-20T20:28:52.584+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes ...
[2024-01-20T20:28:52.586+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish_result :  Row(system_avg_avg_sentiment_bullish=0.0)
[2024-01-20T20:28:52.594+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish_result :  Row(system_avg_avg_sentiment_bearish=0.0)
[2024-01-20T20:28:52.601+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral_result :  Row(system_avg_avg_sentiment_neutral=0.0)
[2024-01-20T20:28:52.602+0000] {spark_submit.py:571} INFO - # Obtenez les valeurs moyennes n√©cessaires
[2024-01-20T20:28:52.611+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish :  0.0
[2024-01-20T20:28:52.618+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish :  0.0
[2024-01-20T20:28:52.620+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral :  0.0
[2024-01-20T20:28:52.620+0000] {spark_submit.py:571} INFO - # Ex√©cutez les requ√™tes...
[2024-01-20T20:28:52.622+0000] {spark_submit.py:571} INFO - max_price :  None
[2024-01-20T20:28:52.623+0000] {spark_submit.py:571} INFO - min_price :  None
[2024-01-20T20:28:52.624+0000] {spark_submit.py:571} INFO - model_path :  /opt/***/shared_volume/best_model_sen.h5
[2024-01-20T20:28:53.581+0000] {spark_submit.py:571} INFO - [[0.]]
[2024-01-20T20:28:53.583+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T20:28:53.584+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 220, in <module>
[2024-01-20T20:28:53.584+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T20:28:53.584+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 187, in batch_processing
[2024-01-20T20:28:53.585+0000] {spark_submit.py:571} INFO - predicted_bitcoin_price = predict_next_day(prices_array1, loaded_model, scaler)
[2024-01-20T20:28:53.585+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 24, in predict_next_day
[2024-01-20T20:28:53.586+0000] {spark_submit.py:571} INFO - last_days = data.reshape(1, time_step, 1)
[2024-01-20T20:28:53.586+0000] {spark_submit.py:571} INFO - ValueError: cannot reshape array of size 1 into shape (1,5,1)
[2024-01-20T20:28:54.730+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T20:28:54.735+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T202835, end_date=20240120T202854
[2024-01-20T20:28:54.751+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1258 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 31)
[2024-01-20T20:28:54.779+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T20:28:54.796+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T20:53:41.210+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T20:53:41.217+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T20:53:41.218+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T20:53:41.233+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T20:53:41.236+0000] {standard_task_runner.py:57} INFO - Started process 31 to run task
[2024-01-20T20:53:41.243+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1261', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmpxu5gyn54']
[2024-01-20T20:53:41.248+0000] {standard_task_runner.py:85} INFO - Job 1261: Subtask submit_spark_job
[2024-01-20T20:53:41.336+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T20:53:41.434+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T20:53:41.444+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T20:53:41.446+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T20:53:41.573+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T20:53:43.437+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T20:53:43.551+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T20:53:43.551+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T20:53:43.556+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T20:53:43.557+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-d2bde116-a762-4015-b192-288d151fd78e;1.0
[2024-01-20T20:53:43.557+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T20:53:43.650+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T20:53:43.677+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T20:53:43.705+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T20:53:43.738+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T20:53:43.760+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T20:53:43.778+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T20:53:43.791+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T20:53:43.805+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T20:53:43.817+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T20:53:43.833+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T20:53:43.845+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T20:53:43.856+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T20:53:43.867+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T20:53:43.879+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T20:53:43.891+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T20:53:43.903+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T20:53:43.929+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T20:53:43.937+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T20:53:43.944+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T20:53:43.971+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 400ms :: artifacts dl 15ms
[2024-01-20T20:53:43.972+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T20:53:43.972+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T20:53:43.973+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T20:53:43.973+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T20:53:43.974+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T20:53:43.974+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T20:53:43.975+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T20:53:43.975+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T20:53:43.975+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T20:53:43.976+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T20:53:43.976+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T20:53:43.977+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T20:53:43.977+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T20:53:43.977+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T20:53:43.978+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T20:53:43.978+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T20:53:43.978+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T20:53:43.979+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T20:53:43.979+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T20:53:43.979+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T20:53:43.980+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:53:43.980+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T20:53:43.980+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T20:53:43.980+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:53:43.981+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T20:53:43.981+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T20:53:43.981+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-d2bde116-a762-4015-b192-288d151fd78e
[2024-01-20T20:53:43.982+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T20:53:43.991+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/10ms)
[2024-01-20T20:53:44.156+0000] {spark_submit.py:571} INFO - 24/01/20 20:53:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T20:53:54.464+0000] {spark_submit.py:571} INFO - 2024-01-20 20:53:54.463344: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T20:53:54.502+0000] {spark_submit.py:571} INFO - 2024-01-20 20:53:54.502142: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T20:53:54.912+0000] {spark_submit.py:571} INFO - 2024-01-20 20:53:54.912180: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T20:53:54.913+0000] {spark_submit.py:571} INFO - 2024-01-20 20:53:54.912648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T20:53:54.970+0000] {spark_submit.py:571} INFO - 2024-01-20 20:53:54.969809: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T20:53:55.184+0000] {spark_submit.py:571} INFO - 2024-01-20 20:53:55.184447: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T20:53:55.190+0000] {spark_submit.py:571} INFO - 2024-01-20 20:53:55.190502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T20:53:55.191+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T20:53:56.653+0000] {spark_submit.py:571} INFO - 2024-01-20 20:53:56.653602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T20:53:59.994+0000] {spark_submit.py:571} INFO - 24/01/20 20:53:59 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T20:54:00.024+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO ResourceUtils: ==============================================================
[2024-01-20T20:54:00.025+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T20:54:00.025+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO ResourceUtils: ==============================================================
[2024-01-20T20:54:00.026+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T20:54:00.053+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T20:54:00.114+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T20:54:00.115+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T20:54:00.175+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T20:54:00.176+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T20:54:00.176+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T20:54:00.176+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T20:54:00.177+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T20:54:00.438+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO Utils: Successfully started service 'sparkDriver' on port 44501.
[2024-01-20T20:54:00.470+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T20:54:00.503+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T20:54:00.518+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T20:54:00.518+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T20:54:00.521+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T20:54:00.543+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-83bdad48-4761-4d3f-a53c-04d30b388d13
[2024-01-20T20:54:00.557+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T20:54:00.570+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T20:54:00.706+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T20:54:00.800+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T20:54:00.830+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:44501/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705784039977
[2024-01-20T20:54:00.831+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:44501/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705784039977
[2024-01-20T20:54:00.831+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:44501/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705784039977
[2024-01-20T20:54:00.832+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:44501/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705784039977
[2024-01-20T20:54:00.832+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:44501/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705784039977
[2024-01-20T20:54:00.832+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:44501/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705784039977
[2024-01-20T20:54:00.833+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:44501/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705784039977
[2024-01-20T20:54:00.833+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:44501/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705784039977
[2024-01-20T20:54:00.833+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:44501/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705784039977
[2024-01-20T20:54:00.833+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:44501/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705784039977
[2024-01-20T20:54:00.834+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:44501/jars/com.typesafe_config-1.4.1.jar with timestamp 1705784039977
[2024-01-20T20:54:00.834+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:44501/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705784039977
[2024-01-20T20:54:00.835+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:44501/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705784039977
[2024-01-20T20:54:00.835+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:44501/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705784039977
[2024-01-20T20:54:00.836+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:44501/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705784039977
[2024-01-20T20:54:00.836+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:44501/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705784039977
[2024-01-20T20:54:00.836+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:44501/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705784039977
[2024-01-20T20:54:00.837+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:44501/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705784039977
[2024-01-20T20:54:00.837+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:44501/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705784039977
[2024-01-20T20:54:00.837+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:44501/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705784039977
[2024-01-20T20:54:00.838+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T20:54:00.874+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:44501/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705784039977
[2024-01-20T20:54:00.876+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T20:54:00.920+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:44501/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705784039977
[2024-01-20T20:54:00.921+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T20:54:00.997+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:44501/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705784039977
[2024-01-20T20:54:00.997+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T20:54:01.213+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:44501/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705784039977
[2024-01-20T20:54:01.214+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T20:54:01.219+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:44501/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705784039977
[2024-01-20T20:54:01.219+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T20:54:01.232+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:44501/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705784039977
[2024-01-20T20:54:01.233+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T20:54:01.239+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:44501/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705784039977
[2024-01-20T20:54:01.240+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T20:54:01.269+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:44501/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705784039977
[2024-01-20T20:54:01.269+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T20:54:01.276+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:44501/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705784039977
[2024-01-20T20:54:01.277+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T20:54:01.294+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:44501/files/com.typesafe_config-1.4.1.jar with timestamp 1705784039977
[2024-01-20T20:54:01.295+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.typesafe_config-1.4.1.jar
[2024-01-20T20:54:01.300+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:44501/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705784039977
[2024-01-20T20:54:01.302+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T20:54:01.305+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:44501/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705784039977
[2024-01-20T20:54:01.307+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T20:54:01.310+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:44501/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705784039977
[2024-01-20T20:54:01.311+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T20:54:01.321+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:44501/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705784039977
[2024-01-20T20:54:01.322+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T20:54:01.326+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:44501/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705784039977
[2024-01-20T20:54:01.327+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T20:54:01.330+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:44501/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705784039977
[2024-01-20T20:54:01.331+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T20:54:01.336+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:44501/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705784039977
[2024-01-20T20:54:01.338+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T20:54:01.341+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:44501/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705784039977
[2024-01-20T20:54:01.342+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-e45f36c1-d57b-4a56-819a-ac256d24bbcf/userFiles-9a678ca3-a4e4-4327-8f6e-570878ff08a0/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T20:54:01.482+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T20:54:01.541+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 40 ms (0 ms spent in bootstraps)
[2024-01-20T20:54:01.804+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120205401-0018
[2024-01-20T20:54:01.812+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33149.
[2024-01-20T20:54:01.813+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:33149
[2024-01-20T20:54:01.814+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T20:54:01.819+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 33149, None)
[2024-01-20T20:54:01.887+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:33149 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 33149, None)
[2024-01-20T20:54:01.890+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 33149, None)
[2024-01-20T20:54:01.891+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 33149, None)
[2024-01-20T20:54:01.895+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120205401-0018/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T20:54:01.897+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120205401-0018/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T20:54:02.172+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:02 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T20:54:02.400+0000] {spark_submit.py:571} INFO - 24/01/20 20:54:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120205401-0018/0 is now RUNNING
[2024-01-20T20:54:02.514+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T20:54:02.610+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T20:54:02.621+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T20:54:02.792+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T20:54:02.797+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T20:54:02.797+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T20:54:02.798+0000] {spark_submit.py:571} INFO - start_of_day_str : 2024-01-19 00:00:00
[2024-01-20T20:54:02.798+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes
[2024-01-20T20:54:02.819+0000] {spark_submit.py:571} INFO - first_timestamp_result : Row(bitcoin_price=41800.5678)
[2024-01-20T20:54:02.823+0000] {spark_submit.py:571} INFO - last_timestamp_result : Row(bitcoin_price=41800.5678)
[2024-01-20T20:54:02.878+0000] {spark_submit.py:571} INFO - sum_posts_result : Row(system_sum_posts_last_5_minutes=554)
[2024-01-20T20:54:02.879+0000] {spark_submit.py:571} INFO - Obtenez les valeurs n√©cessaires
[2024-01-20T20:54:02.879+0000] {spark_submit.py:571} INFO - first_timestamp_bitcoin_price :  41800.5678
[2024-01-20T20:54:02.879+0000] {spark_submit.py:571} INFO - last_timestamp_bitcoin_price :  41800.5678
[2024-01-20T20:54:02.880+0000] {spark_submit.py:571} INFO - sum_posts_last_day :  554
[2024-01-20T20:54:02.881+0000] {spark_submit.py:571} INFO - change_from_kafka : 0.0
[2024-01-20T20:54:02.881+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes ...
[2024-01-20T20:54:02.881+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish_result :  Row(system_avg_avg_sentiment_bullish=0.0)
[2024-01-20T20:54:02.885+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish_result :  Row(system_avg_avg_sentiment_bearish=0.0)
[2024-01-20T20:54:02.888+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral_result :  Row(system_avg_avg_sentiment_neutral=0.0)
[2024-01-20T20:54:02.888+0000] {spark_submit.py:571} INFO - # Obtenez les valeurs moyennes n√©cessaires
[2024-01-20T20:54:02.889+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish :  0.0
[2024-01-20T20:54:02.889+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish :  0.0
[2024-01-20T20:54:02.890+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral :  0.0
[2024-01-20T20:54:02.890+0000] {spark_submit.py:571} INFO - # Ex√©cutez les requ√™tes...
[2024-01-20T20:54:02.902+0000] {spark_submit.py:571} INFO - max_price :  None
[2024-01-20T20:54:02.903+0000] {spark_submit.py:571} INFO - min_price :  None
[2024-01-20T20:54:02.903+0000] {spark_submit.py:571} INFO - model_path :  /opt/***/shared_volume/best_model_sen.h5
[2024-01-20T20:54:02.918+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.2.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
[2024-01-20T20:54:02.918+0000] {spark_submit.py:571} INFO - https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
[2024-01-20T20:54:02.919+0000] {spark_submit.py:571} INFO - warnings.warn(
[2024-01-20T20:54:03.876+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T20:54:03.877+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 227, in <module>
[2024-01-20T20:54:03.877+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T20:54:03.877+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 188, in batch_processing
[2024-01-20T20:54:03.878+0000] {spark_submit.py:571} INFO - scaled_data = scaler.transform(historical_data)
[2024-01-20T20:54:03.878+0000] {spark_submit.py:571} INFO - File "/home/***/.local/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 273, in wrapped
[2024-01-20T20:54:03.879+0000] {spark_submit.py:571} INFO - data_to_wrap = f(self, X, *args, **kwargs)
[2024-01-20T20:54:03.879+0000] {spark_submit.py:571} INFO - File "/home/***/.local/lib/python3.9/site-packages/sklearn/preprocessing/_data.py", line 534, in transform
[2024-01-20T20:54:03.879+0000] {spark_submit.py:571} INFO - X = self._validate_data(
[2024-01-20T20:54:03.880+0000] {spark_submit.py:571} INFO - File "/home/***/.local/lib/python3.9/site-packages/sklearn/base.py", line 654, in _validate_data
[2024-01-20T20:54:03.880+0000] {spark_submit.py:571} INFO - self._check_n_features(X, reset=reset)
[2024-01-20T20:54:03.880+0000] {spark_submit.py:571} INFO - File "/home/***/.local/lib/python3.9/site-packages/sklearn/base.py", line 443, in _check_n_features
[2024-01-20T20:54:03.881+0000] {spark_submit.py:571} INFO - raise ValueError(
[2024-01-20T20:54:03.881+0000] {spark_submit.py:571} INFO - ValueError: X has 4 features, but MinMaxScaler is expecting 1 features as input.
[2024-01-20T20:54:04.822+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T20:54:04.829+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T205341, end_date=20240120T205404
[2024-01-20T20:54:04.847+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1261 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 31)
[2024-01-20T20:54:04.860+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T20:54:04.874+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T21:10:35.694+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T21:10:35.700+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T21:10:35.701+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T21:10:35.714+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T21:10:35.717+0000] {standard_task_runner.py:57} INFO - Started process 1801 to run task
[2024-01-20T21:10:35.719+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1266', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmp7p224oku']
[2024-01-20T21:10:35.720+0000] {standard_task_runner.py:85} INFO - Job 1266: Subtask submit_spark_job
[2024-01-20T21:10:35.760+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T21:10:35.831+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T21:10:35.875+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T21:10:35.877+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T21:10:35.984+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T21:10:37.329+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T21:10:37.463+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T21:10:37.464+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T21:10:37.467+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T21:10:37.468+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-80df3272-87cd-4af3-817d-5440806b55d0;1.0
[2024-01-20T21:10:37.468+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T21:10:37.541+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T21:10:37.561+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T21:10:37.585+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T21:10:37.614+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T21:10:37.641+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T21:10:37.660+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T21:10:37.679+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T21:10:37.691+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T21:10:37.705+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T21:10:37.717+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T21:10:37.728+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T21:10:37.740+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T21:10:37.750+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T21:10:37.761+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T21:10:37.772+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T21:10:37.783+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T21:10:37.809+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T21:10:37.819+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T21:10:37.826+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T21:10:37.858+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 373ms :: artifacts dl 17ms
[2024-01-20T21:10:37.859+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T21:10:37.860+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T21:10:37.860+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T21:10:37.860+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T21:10:37.861+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T21:10:37.861+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T21:10:37.862+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T21:10:37.862+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T21:10:37.863+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T21:10:37.863+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T21:10:37.863+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T21:10:37.864+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T21:10:37.864+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T21:10:37.864+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T21:10:37.865+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T21:10:37.865+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T21:10:37.866+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T21:10:37.866+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T21:10:37.867+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T21:10:37.867+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T21:10:37.867+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:10:37.868+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T21:10:37.868+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T21:10:37.868+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:10:37.869+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T21:10:37.869+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:10:37.870+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-80df3272-87cd-4af3-817d-5440806b55d0
[2024-01-20T21:10:37.870+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T21:10:37.876+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/9ms)
[2024-01-20T21:10:38.063+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T21:10:46.284+0000] {spark_submit.py:571} INFO - 2024-01-20 21:10:46.284459: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T21:10:46.286+0000] {spark_submit.py:571} INFO - 2024-01-20 21:10:46.286545: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T21:10:46.316+0000] {spark_submit.py:571} INFO - 2024-01-20 21:10:46.316695: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T21:10:46.317+0000] {spark_submit.py:571} INFO - 2024-01-20 21:10:46.316755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T21:10:46.319+0000] {spark_submit.py:571} INFO - 2024-01-20 21:10:46.318271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T21:10:46.325+0000] {spark_submit.py:571} INFO - 2024-01-20 21:10:46.324895: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T21:10:46.325+0000] {spark_submit.py:571} INFO - 2024-01-20 21:10:46.325163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T21:10:46.326+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T21:10:47.066+0000] {spark_submit.py:571} INFO - 2024-01-20 21:10:47.066689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T21:10:49.193+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T21:10:49.217+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO ResourceUtils: ==============================================================
[2024-01-20T21:10:49.218+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T21:10:49.218+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO ResourceUtils: ==============================================================
[2024-01-20T21:10:49.219+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T21:10:49.244+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T21:10:49.259+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T21:10:49.260+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T21:10:49.311+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T21:10:49.313+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T21:10:49.313+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T21:10:49.313+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T21:10:49.314+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T21:10:49.491+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Successfully started service 'sparkDriver' on port 42591.
[2024-01-20T21:10:49.518+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T21:10:49.551+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T21:10:49.565+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T21:10:49.566+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T21:10:49.569+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T21:10:49.589+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-75c3c26c-4396-4a05-8b4d-883d30122de6
[2024-01-20T21:10:49.603+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T21:10:49.616+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T21:10:49.729+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T21:10:49.787+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T21:10:49.817+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:42591/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.818+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:42591/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.819+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:42591/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.819+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:42591/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.820+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:42591/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.820+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:42591/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705785049185
[2024-01-20T21:10:49.821+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:42591/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705785049185
[2024-01-20T21:10:49.821+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:42591/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705785049185
[2024-01-20T21:10:49.821+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:42591/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.821+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:42591/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.822+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:42591/jars/com.typesafe_config-1.4.1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.822+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:42591/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705785049185
[2024-01-20T21:10:49.823+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:42591/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705785049185
[2024-01-20T21:10:49.823+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:42591/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705785049185
[2024-01-20T21:10:49.825+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:42591/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705785049185
[2024-01-20T21:10:49.825+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:42591/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.826+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:42591/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705785049185
[2024-01-20T21:10:49.826+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:42591/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705785049185
[2024-01-20T21:10:49.826+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:42591/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.827+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:42591/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.827+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T21:10:49.832+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:42591/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.833+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T21:10:49.838+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:42591/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.839+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T21:10:49.842+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:42591/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.843+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T21:10:49.853+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:42591/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.853+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T21:10:49.856+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:42591/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705785049185
[2024-01-20T21:10:49.857+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T21:10:49.860+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:42591/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705785049185
[2024-01-20T21:10:49.861+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T21:10:49.869+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:42591/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705785049185
[2024-01-20T21:10:49.870+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T21:10:49.872+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:42591/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.873+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T21:10:49.876+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:42591/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.877+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T21:10:49.882+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:42591/files/com.typesafe_config-1.4.1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.883+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.typesafe_config-1.4.1.jar
[2024-01-20T21:10:49.886+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:42591/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705785049185
[2024-01-20T21:10:49.887+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T21:10:49.889+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:42591/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705785049185
[2024-01-20T21:10:49.890+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T21:10:49.894+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:42591/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705785049185
[2024-01-20T21:10:49.894+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T21:10:49.897+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:42591/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705785049185
[2024-01-20T21:10:49.898+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T21:10:49.900+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:42591/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705785049185
[2024-01-20T21:10:49.901+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T21:10:49.904+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:42591/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705785049185
[2024-01-20T21:10:49.905+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T21:10:49.907+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:42591/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705785049185
[2024-01-20T21:10:49.908+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T21:10:49.911+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:42591/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705785049185
[2024-01-20T21:10:49.911+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-f4f324a2-3904-4a4b-a952-ed29e96d9f91/userFiles-23ea3e28-b89a-4b8f-8422-d8fea7752b75/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T21:10:49.973+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T21:10:50.004+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 20 ms (0 ms spent in bootstraps)
[2024-01-20T21:10:50.082+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120211050-0023
[2024-01-20T21:10:50.083+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120211050-0023/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T21:10:50.085+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120211050-0023/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T21:10:50.089+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37897.
[2024-01-20T21:10:50.089+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:37897
[2024-01-20T21:10:50.090+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T21:10:50.096+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 37897, None)
[2024-01-20T21:10:50.100+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:37897 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 37897, None)
[2024-01-20T21:10:50.104+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 37897, None)
[2024-01-20T21:10:50.105+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 37897, None)
[2024-01-20T21:10:50.273+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T21:10:50.278+0000] {spark_submit.py:571} INFO - 24/01/20 21:10:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120211050-0023/0 is now RUNNING
[2024-01-20T21:10:50.473+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T21:10:50.481+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T21:10:50.485+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T21:10:50.524+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T21:10:50.525+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T21:10:50.525+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T21:10:50.526+0000] {spark_submit.py:571} INFO - start_of_day_str : 2024-01-19 00:00:00
[2024-01-20T21:10:50.526+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes
[2024-01-20T21:10:50.528+0000] {spark_submit.py:571} INFO - first_timestamp_result : Row(close_price=42100.0)
[2024-01-20T21:10:50.530+0000] {spark_submit.py:571} INFO - last_timestamp_result : Row(bitcoin_price=41800.5678)
[2024-01-20T21:10:50.535+0000] {spark_submit.py:571} INFO - sum_posts_result : Row(system_sum_posts_last_5_minutes=554)
[2024-01-20T21:10:50.536+0000] {spark_submit.py:571} INFO - Obtenez les valeurs n√©cessaires
[2024-01-20T21:10:50.537+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T21:10:50.537+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 227, in <module>
[2024-01-20T21:10:50.537+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T21:10:50.538+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 125, in batch_processing
[2024-01-20T21:10:50.538+0000] {spark_submit.py:571} INFO - first_timestamp_bitcoin_price = first_timestamp_result.bitcoin_price if first_timestamp_result else 0.0
[2024-01-20T21:10:50.539+0000] {spark_submit.py:571} INFO - AttributeError: 'Row' object has no attribute 'bitcoin_price'
[2024-01-20T21:10:51.284+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T21:10:51.289+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T211035, end_date=20240120T211051
[2024-01-20T21:10:51.305+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1266 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 1801)
[2024-01-20T21:10:51.316+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T21:10:51.333+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T21:11:47.048+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T21:11:47.055+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T21:11:47.056+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T21:11:47.069+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T21:11:47.073+0000] {standard_task_runner.py:57} INFO - Started process 2337 to run task
[2024-01-20T21:11:47.075+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1268', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmpxq2iep4d']
[2024-01-20T21:11:47.076+0000] {standard_task_runner.py:85} INFO - Job 1268: Subtask submit_spark_job
[2024-01-20T21:11:47.116+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T21:11:47.178+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T21:11:47.187+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T21:11:47.188+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T21:11:47.295+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T21:11:48.624+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T21:11:48.757+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T21:11:48.758+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T21:11:48.761+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T21:11:48.761+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-c31e3023-452c-44ac-b49c-b2bceda2909c;1.0
[2024-01-20T21:11:48.762+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T21:11:48.844+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T21:11:48.869+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T21:11:48.891+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T21:11:48.921+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T21:11:48.942+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T21:11:48.960+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T21:11:48.973+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T21:11:48.986+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T21:11:49.001+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T21:11:49.017+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T21:11:49.028+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T21:11:49.041+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T21:11:49.050+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T21:11:49.061+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T21:11:49.076+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T21:11:49.087+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T21:11:49.111+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T21:11:49.121+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T21:11:49.128+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T21:11:49.160+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 383ms :: artifacts dl 16ms
[2024-01-20T21:11:49.161+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T21:11:49.162+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T21:11:49.162+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T21:11:49.163+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T21:11:49.163+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T21:11:49.163+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T21:11:49.164+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T21:11:49.164+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T21:11:49.165+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T21:11:49.165+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T21:11:49.166+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T21:11:49.166+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T21:11:49.167+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T21:11:49.167+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T21:11:49.167+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T21:11:49.168+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T21:11:49.169+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T21:11:49.169+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T21:11:49.170+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T21:11:49.170+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T21:11:49.170+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:11:49.171+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T21:11:49.171+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T21:11:49.172+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:11:49.172+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T21:11:49.173+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:11:49.174+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-c31e3023-452c-44ac-b49c-b2bceda2909c
[2024-01-20T21:11:49.174+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T21:11:49.178+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/11ms)
[2024-01-20T21:11:49.335+0000] {spark_submit.py:571} INFO - 24/01/20 21:11:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T21:11:57.690+0000] {spark_submit.py:571} INFO - 2024-01-20 21:11:57.690072: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T21:11:57.692+0000] {spark_submit.py:571} INFO - 2024-01-20 21:11:57.692170: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T21:11:57.717+0000] {spark_submit.py:571} INFO - 2024-01-20 21:11:57.717680: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T21:11:57.718+0000] {spark_submit.py:571} INFO - 2024-01-20 21:11:57.717731: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T21:11:57.718+0000] {spark_submit.py:571} INFO - 2024-01-20 21:11:57.718704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T21:11:57.724+0000] {spark_submit.py:571} INFO - 2024-01-20 21:11:57.724395: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T21:11:57.724+0000] {spark_submit.py:571} INFO - 2024-01-20 21:11:57.724588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T21:11:57.725+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T21:11:58.407+0000] {spark_submit.py:571} INFO - 2024-01-20 21:11:58.406888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T21:12:00.134+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T21:12:00.151+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO ResourceUtils: ==============================================================
[2024-01-20T21:12:00.151+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T21:12:00.152+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO ResourceUtils: ==============================================================
[2024-01-20T21:12:00.152+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T21:12:00.171+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T21:12:00.182+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T21:12:00.183+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T21:12:00.229+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T21:12:00.229+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T21:12:00.230+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T21:12:00.230+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T21:12:00.231+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T21:12:00.407+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Successfully started service 'sparkDriver' on port 36267.
[2024-01-20T21:12:00.430+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T21:12:00.457+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T21:12:00.469+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T21:12:00.470+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T21:12:00.472+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T21:12:00.489+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b712c7b1-1425-43d0-ac38-f1b10952bee0
[2024-01-20T21:12:00.503+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T21:12:00.515+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T21:12:00.625+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T21:12:00.676+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T21:12:00.704+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:36267/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.705+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:36267/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.705+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:36267/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.706+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:36267/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.706+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:36267/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.707+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:36267/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705785120125
[2024-01-20T21:12:00.707+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:36267/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705785120125
[2024-01-20T21:12:00.708+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:36267/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705785120125
[2024-01-20T21:12:00.708+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:36267/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.709+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:36267/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.709+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:36267/jars/com.typesafe_config-1.4.1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.709+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:36267/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705785120125
[2024-01-20T21:12:00.710+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:36267/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705785120125
[2024-01-20T21:12:00.710+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:36267/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705785120125
[2024-01-20T21:12:00.711+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:36267/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705785120125
[2024-01-20T21:12:00.711+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:36267/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.711+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:36267/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705785120125
[2024-01-20T21:12:00.712+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:36267/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705785120125
[2024-01-20T21:12:00.712+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:36267/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.713+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:36267/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.713+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T21:12:00.719+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:36267/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.720+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T21:12:00.724+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:36267/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.725+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T21:12:00.729+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:36267/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.729+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T21:12:00.738+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:36267/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.739+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T21:12:00.742+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:36267/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705785120125
[2024-01-20T21:12:00.743+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T21:12:00.746+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:36267/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705785120125
[2024-01-20T21:12:00.747+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T21:12:00.750+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:36267/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705785120125
[2024-01-20T21:12:00.750+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T21:12:00.756+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:36267/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.756+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T21:12:00.760+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:36267/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.761+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T21:12:00.766+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:36267/files/com.typesafe_config-1.4.1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.766+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.typesafe_config-1.4.1.jar
[2024-01-20T21:12:00.770+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:36267/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705785120125
[2024-01-20T21:12:00.770+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T21:12:00.773+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:36267/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705785120125
[2024-01-20T21:12:00.774+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T21:12:00.778+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:36267/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705785120125
[2024-01-20T21:12:00.779+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T21:12:00.781+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:36267/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705785120125
[2024-01-20T21:12:00.782+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T21:12:00.785+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:36267/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705785120125
[2024-01-20T21:12:00.785+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T21:12:00.788+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:36267/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705785120125
[2024-01-20T21:12:00.789+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T21:12:00.791+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:36267/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705785120125
[2024-01-20T21:12:00.792+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T21:12:00.794+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:36267/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705785120125
[2024-01-20T21:12:00.795+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-9421db7a-4861-443b-9fa1-c99552cfc247/userFiles-cd0bb36a-c0d2-4f67-b03e-70ad0cd50850/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T21:12:00.864+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T21:12:00.897+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 20 ms (0 ms spent in bootstraps)
[2024-01-20T21:12:00.968+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120211200-0025
[2024-01-20T21:12:00.970+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120211200-0025/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T21:12:00.972+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120211200-0025/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T21:12:00.974+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46709.
[2024-01-20T21:12:00.975+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:46709
[2024-01-20T21:12:00.976+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T21:12:00.981+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 46709, None)
[2024-01-20T21:12:00.984+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:46709 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 46709, None)
[2024-01-20T21:12:00.986+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 46709, None)
[2024-01-20T21:12:00.987+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 46709, None)
[2024-01-20T21:12:01.056+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120211200-0025/0 is now RUNNING
[2024-01-20T21:12:01.188+0000] {spark_submit.py:571} INFO - 24/01/20 21:12:01 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T21:12:01.409+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T21:12:01.418+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T21:12:01.425+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T21:12:01.465+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T21:12:01.467+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T21:12:01.467+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T21:12:01.468+0000] {spark_submit.py:571} INFO - start_of_day_str : 2024-01-19 00:00:00
[2024-01-20T21:12:01.469+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes
[2024-01-20T21:12:01.470+0000] {spark_submit.py:571} INFO - first_timestamp_result : Row(close_price=42100.0)
[2024-01-20T21:12:01.499+0000] {spark_submit.py:571} INFO - last_timestamp_result : Row(bitcoin_price=41800.5678)
[2024-01-20T21:12:01.503+0000] {spark_submit.py:571} INFO - sum_posts_result : Row(system_sum_posts_last_5_minutes=554)
[2024-01-20T21:12:01.504+0000] {spark_submit.py:571} INFO - Obtenez les valeurs n√©cessaires
[2024-01-20T21:12:01.504+0000] {spark_submit.py:571} INFO - first_timestamp_bitcoin_price :  42100.0
[2024-01-20T21:12:01.505+0000] {spark_submit.py:571} INFO - last_timestamp_bitcoin_price :  41800.5678
[2024-01-20T21:12:01.506+0000] {spark_submit.py:571} INFO - sum_posts_last_day :  554
[2024-01-20T21:12:01.506+0000] {spark_submit.py:571} INFO - change_from_kafka : -0.7112403800475129
[2024-01-20T21:12:01.507+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes ...
[2024-01-20T21:12:01.509+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish_result :  Row(system_avg_avg_sentiment_bullish=0.49302727272727276)
[2024-01-20T21:12:01.513+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish_result :  Row(system_avg_avg_sentiment_bearish=0.44849090909090905)
[2024-01-20T21:12:01.522+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral_result :  Row(system_avg_avg_sentiment_neutral=0.5075363636363637)
[2024-01-20T21:12:01.522+0000] {spark_submit.py:571} INFO - # Obtenez les valeurs moyennes n√©cessaires
[2024-01-20T21:12:01.523+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish :  0.49302727272727276
[2024-01-20T21:12:01.523+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish :  0.44849090909090905
[2024-01-20T21:12:01.524+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral :  0.5075363636363637
[2024-01-20T21:12:01.524+0000] {spark_submit.py:571} INFO - # Ex√©cutez les requ√™tes...
[2024-01-20T21:12:01.530+0000] {spark_submit.py:571} INFO - max_price :  42500.7891
[2024-01-20T21:12:01.531+0000] {spark_submit.py:571} INFO - min_price :  41500.6543
[2024-01-20T21:12:01.532+0000] {spark_submit.py:571} INFO - model_path :  /opt/***/shared_volume/best_model_sen.h5
[2024-01-20T21:12:01.534+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T21:12:01.534+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 227, in <module>
[2024-01-20T21:12:01.535+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T21:12:01.535+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 179, in batch_processing
[2024-01-20T21:12:01.536+0000] {spark_submit.py:571} INFO - prices_data = session.execute(query_prices).all()
[2024-01-20T21:12:01.536+0000] {spark_submit.py:571} INFO - File "cassandra/cluster.py", line 2637, in cassandra.cluster.Session.execute
[2024-01-20T21:12:01.537+0000] {spark_submit.py:571} INFO - File "cassandra/cluster.py", line 4920, in cassandra.cluster.ResponseFuture.result
[2024-01-20T21:12:01.537+0000] {spark_submit.py:571} INFO - cassandra.InvalidRequest: Error from server: code=2200 [Invalid query] message="Undefined column name bitcoin_price in table bigdataproject.batchlayer"
[2024-01-20T21:12:02.297+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T21:12:02.300+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T211147, end_date=20240120T211202
[2024-01-20T21:12:02.311+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1268 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 2337)
[2024-01-20T21:12:02.335+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T21:12:02.349+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T21:16:45.970+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T21:16:45.976+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T21:16:45.976+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T21:16:45.987+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T21:16:45.989+0000] {standard_task_runner.py:57} INFO - Started process 3240 to run task
[2024-01-20T21:16:45.991+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1271', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmpq6g3zqtf']
[2024-01-20T21:16:45.993+0000] {standard_task_runner.py:85} INFO - Job 1271: Subtask submit_spark_job
[2024-01-20T21:16:46.032+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T21:16:46.087+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T21:16:46.094+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T21:16:46.095+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T21:16:46.196+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T21:16:47.488+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T21:16:47.593+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T21:16:47.593+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T21:16:47.596+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T21:16:47.596+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-11089d59-aabe-4679-9539-5839d0f261b5;1.0
[2024-01-20T21:16:47.597+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T21:16:47.675+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T21:16:47.697+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T21:16:47.725+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T21:16:47.752+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T21:16:47.771+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T21:16:47.786+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T21:16:47.805+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T21:16:47.820+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T21:16:47.835+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T21:16:47.851+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T21:16:47.862+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T21:16:47.872+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T21:16:47.888+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T21:16:47.902+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T21:16:47.919+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T21:16:47.936+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T21:16:47.950+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T21:16:47.959+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T21:16:47.967+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T21:16:48.010+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 397ms :: artifacts dl 17ms
[2024-01-20T21:16:48.011+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T21:16:48.011+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T21:16:48.012+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T21:16:48.013+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T21:16:48.013+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T21:16:48.013+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T21:16:48.014+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T21:16:48.014+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T21:16:48.015+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T21:16:48.015+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T21:16:48.016+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T21:16:48.016+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T21:16:48.016+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T21:16:48.017+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T21:16:48.017+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T21:16:48.018+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T21:16:48.018+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T21:16:48.019+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T21:16:48.019+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T21:16:48.020+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T21:16:48.020+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:16:48.021+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T21:16:48.021+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T21:16:48.021+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:16:48.022+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T21:16:48.022+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:16:48.023+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-11089d59-aabe-4679-9539-5839d0f261b5
[2024-01-20T21:16:48.023+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T21:16:48.033+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/13ms)
[2024-01-20T21:16:48.180+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T21:16:56.597+0000] {spark_submit.py:571} INFO - 2024-01-20 21:16:56.596832: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T21:16:56.598+0000] {spark_submit.py:571} INFO - 2024-01-20 21:16:56.598826: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T21:16:56.629+0000] {spark_submit.py:571} INFO - 2024-01-20 21:16:56.629189: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T21:16:56.629+0000] {spark_submit.py:571} INFO - 2024-01-20 21:16:56.629241: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T21:16:56.631+0000] {spark_submit.py:571} INFO - 2024-01-20 21:16:56.631073: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T21:16:56.636+0000] {spark_submit.py:571} INFO - 2024-01-20 21:16:56.636671: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T21:16:56.637+0000] {spark_submit.py:571} INFO - 2024-01-20 21:16:56.636863: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T21:16:56.637+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T21:16:57.450+0000] {spark_submit.py:571} INFO - 2024-01-20 21:16:57.450678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T21:16:59.252+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T21:16:59.268+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO ResourceUtils: ==============================================================
[2024-01-20T21:16:59.268+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T21:16:59.269+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO ResourceUtils: ==============================================================
[2024-01-20T21:16:59.269+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T21:16:59.285+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T21:16:59.294+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T21:16:59.295+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T21:16:59.331+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T21:16:59.332+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T21:16:59.333+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T21:16:59.333+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T21:16:59.334+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T21:16:59.519+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Successfully started service 'sparkDriver' on port 33825.
[2024-01-20T21:16:59.541+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T21:16:59.565+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T21:16:59.576+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T21:16:59.577+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T21:16:59.580+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T21:16:59.595+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7ec57176-5934-416d-a1b5-d230670ece08
[2024-01-20T21:16:59.607+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T21:16:59.618+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T21:16:59.714+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T21:16:59.756+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T21:16:59.781+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:33825/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.782+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:33825/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.783+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:33825/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.783+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:33825/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.784+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:33825/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.784+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:33825/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705785419246
[2024-01-20T21:16:59.784+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:33825/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705785419246
[2024-01-20T21:16:59.785+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:33825/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705785419246
[2024-01-20T21:16:59.785+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:33825/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.786+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:33825/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.786+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:33825/jars/com.typesafe_config-1.4.1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.786+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:33825/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705785419246
[2024-01-20T21:16:59.787+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:33825/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705785419246
[2024-01-20T21:16:59.787+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:33825/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705785419246
[2024-01-20T21:16:59.787+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:33825/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705785419246
[2024-01-20T21:16:59.788+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:33825/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.788+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:33825/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705785419246
[2024-01-20T21:16:59.789+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:33825/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705785419246
[2024-01-20T21:16:59.790+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:33825/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.790+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:33825/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.791+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T21:16:59.798+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:33825/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.799+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T21:16:59.802+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:33825/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.803+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T21:16:59.806+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:33825/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.806+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T21:16:59.814+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:33825/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.814+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T21:16:59.817+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:33825/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705785419246
[2024-01-20T21:16:59.818+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T21:16:59.820+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:33825/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705785419246
[2024-01-20T21:16:59.821+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T21:16:59.823+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:33825/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705785419246
[2024-01-20T21:16:59.824+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T21:16:59.829+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:33825/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.829+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T21:16:59.833+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:33825/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.834+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T21:16:59.839+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:33825/files/com.typesafe_config-1.4.1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.839+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.typesafe_config-1.4.1.jar
[2024-01-20T21:16:59.842+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:33825/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705785419246
[2024-01-20T21:16:59.843+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T21:16:59.845+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:33825/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705785419246
[2024-01-20T21:16:59.846+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T21:16:59.848+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:33825/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705785419246
[2024-01-20T21:16:59.849+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T21:16:59.851+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:33825/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705785419246
[2024-01-20T21:16:59.852+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T21:16:59.854+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:33825/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705785419246
[2024-01-20T21:16:59.854+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T21:16:59.856+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:33825/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705785419246
[2024-01-20T21:16:59.857+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T21:16:59.861+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:33825/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705785419246
[2024-01-20T21:16:59.861+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T21:16:59.868+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:33825/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705785419246
[2024-01-20T21:16:59.868+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-1affee1b-3e85-4b6d-befc-e9ecb58c8bd8/userFiles-0c4198b2-bb25-4e5d-af7a-a7ca06e30237/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T21:16:59.992+0000] {spark_submit.py:571} INFO - 24/01/20 21:16:59 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T21:17:00.024+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 19 ms (0 ms spent in bootstraps)
[2024-01-20T21:17:00.091+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120211700-0028
[2024-01-20T21:17:00.092+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120211700-0028/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T21:17:00.094+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120211700-0028/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T21:17:00.096+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36153.
[2024-01-20T21:17:00.097+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:36153
[2024-01-20T21:17:00.098+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T21:17:00.102+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 36153, None)
[2024-01-20T21:17:00.105+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:36153 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 36153, None)
[2024-01-20T21:17:00.106+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 36153, None)
[2024-01-20T21:17:00.107+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 36153, None)
[2024-01-20T21:17:00.141+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120211700-0028/0 is now RUNNING
[2024-01-20T21:17:00.277+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:00 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T21:17:00.472+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T21:17:00.504+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T21:17:00.506+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T21:17:00.538+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T21:17:00.540+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T21:17:00.541+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T21:17:00.542+0000] {spark_submit.py:571} INFO - start_of_day_str : 2024-01-19 00:00:00
[2024-01-20T21:17:00.542+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes
[2024-01-20T21:17:00.545+0000] {spark_submit.py:571} INFO - first_timestamp_result : Row(close_price=42100.0)
[2024-01-20T21:17:00.550+0000] {spark_submit.py:571} INFO - last_timestamp_result : Row(bitcoin_price=41800.5678)
[2024-01-20T21:17:00.554+0000] {spark_submit.py:571} INFO - sum_posts_result : Row(system_sum_posts_last_5_minutes=554)
[2024-01-20T21:17:00.555+0000] {spark_submit.py:571} INFO - Obtenez les valeurs n√©cessaires
[2024-01-20T21:17:00.555+0000] {spark_submit.py:571} INFO - first_timestamp_bitcoin_price :  42100.0
[2024-01-20T21:17:00.556+0000] {spark_submit.py:571} INFO - last_timestamp_bitcoin_price :  41800.5678
[2024-01-20T21:17:00.556+0000] {spark_submit.py:571} INFO - sum_posts_last_day :  554
[2024-01-20T21:17:00.557+0000] {spark_submit.py:571} INFO - change_from_kafka : -0.7112403800475129
[2024-01-20T21:17:00.557+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes ...
[2024-01-20T21:17:00.557+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish_result :  Row(system_avg_avg_sentiment_bullish=0.49302727272727276)
[2024-01-20T21:17:00.562+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish_result :  Row(system_avg_avg_sentiment_bearish=0.44849090909090905)
[2024-01-20T21:17:00.567+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral_result :  Row(system_avg_avg_sentiment_neutral=0.5075363636363637)
[2024-01-20T21:17:00.568+0000] {spark_submit.py:571} INFO - # Obtenez les valeurs moyennes n√©cessaires
[2024-01-20T21:17:00.569+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish :  0.49302727272727276
[2024-01-20T21:17:00.569+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish :  0.44849090909090905
[2024-01-20T21:17:00.570+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral :  0.5075363636363637
[2024-01-20T21:17:00.570+0000] {spark_submit.py:571} INFO - # Ex√©cutez les requ√™tes...
[2024-01-20T21:17:00.575+0000] {spark_submit.py:571} INFO - max_price :  42500.7891
[2024-01-20T21:17:00.576+0000] {spark_submit.py:571} INFO - min_price :  41500.6543
[2024-01-20T21:17:00.576+0000] {spark_submit.py:571} INFO - model_path :  /opt/***/shared_volume/best_model_sen.h5
[2024-01-20T21:17:00.583+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.2.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
[2024-01-20T21:17:00.584+0000] {spark_submit.py:571} INFO - https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
[2024-01-20T21:17:00.584+0000] {spark_submit.py:571} INFO - warnings.warn(
[2024-01-20T21:17:00.585+0000] {spark_submit.py:571} INFO - load model ...
[2024-01-20T21:17:01.216+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-20T21:17:01.216+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 229, in <module>
[2024-01-20T21:17:01.217+0000] {spark_submit.py:571} INFO - batch_processing()
[2024-01-20T21:17:01.217+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 187, in batch_processing
[2024-01-20T21:17:01.218+0000] {spark_submit.py:571} INFO - historical_data = [(row.bitcoin_price, row.avg_sentiment_bullish, row.avg_sentiment_bearish, row.avg_sentiment_neutral) for row in prices_data]
[2024-01-20T21:17:01.218+0000] {spark_submit.py:571} INFO - File "/opt/***/shared_volume/spark_batch.py", line 187, in <listcomp>
[2024-01-20T21:17:01.219+0000] {spark_submit.py:571} INFO - historical_data = [(row.bitcoin_price, row.avg_sentiment_bullish, row.avg_sentiment_bearish, row.avg_sentiment_neutral) for row in prices_data]
[2024-01-20T21:17:01.219+0000] {spark_submit.py:571} INFO - AttributeError: 'Row' object has no attribute 'bitcoin_price'
[2024-01-20T21:17:02.086+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.
[2024-01-20T21:17:02.091+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T211645, end_date=20240120T211702
[2024-01-20T21:17:02.105+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 1271 for task submit_spark_job (Cannot execute: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py. Error code is: 1.; 3240)
[2024-01-20T21:17:02.113+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-20T21:17:02.135+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-20T21:17:47.228+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T21:17:47.234+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [queued]>
[2024-01-20T21:17:47.235+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-20T21:17:47.248+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): submit_spark_job> on 2024-01-18 23:59:00+00:00
[2024-01-20T21:17:47.250+0000] {standard_task_runner.py:57} INFO - Started process 3889 to run task
[2024-01-20T21:17:47.253+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'BatchLayer', 'submit_spark_job', 'scheduled__2024-01-18T23:59:00+00:00', '--job-id', '1273', '--raw', '--subdir', 'DAGS_FOLDER/spark_Batch_dag.py', '--cfg-path', '/tmp/tmpxb_8fk4z']
[2024-01-20T21:17:47.254+0000] {standard_task_runner.py:85} INFO - Job 1273: Subtask submit_spark_job
[2024-01-20T21:17:47.299+0000] {task_command.py:415} INFO - Running <TaskInstance: BatchLayer.submit_spark_job scheduled__2024-01-18T23:59:00+00:00 [running]> on host 45c5bfab8aa1
[2024-01-20T21:17:47.366+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='BatchLayer' AIRFLOW_CTX_TASK_ID='submit_spark_job' AIRFLOW_CTX_EXECUTION_DATE='2024-01-18T23:59:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-18T23:59:00+00:00'
[2024-01-20T21:17:47.374+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-20T21:17:47.375+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://172.19.0.5:7077 --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 --name arrow-spark --queue root.default --deploy-mode client /opt/***/shared_volume/spark_batch.py
[2024-01-20T21:17:47.486+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-20T21:17:48.641+0000] {spark_submit.py:571} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-01-20T21:17:48.772+0000] {spark_submit.py:571} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-01-20T21:17:48.772+0000] {spark_submit.py:571} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-01-20T21:17:48.775+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2024-01-20T21:17:48.775+0000] {spark_submit.py:571} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-49444278-b937-4e18-88b6-04caaae8cb5f;1.0
[2024-01-20T21:17:48.776+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T21:17:48.838+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central
[2024-01-20T21:17:48.859+0000] {spark_submit.py:571} INFO - found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central
[2024-01-20T21:17:48.879+0000] {spark_submit.py:571} INFO - found org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central
[2024-01-20T21:17:48.909+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-core-shaded;4.13.0 in central
[2024-01-20T21:17:48.928+0000] {spark_submit.py:571} INFO - found com.datastax.oss#native-protocol;1.5.0 in central
[2024-01-20T21:17:48.940+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2024-01-20T21:17:48.951+0000] {spark_submit.py:571} INFO - found com.typesafe#config;1.4.1 in central
[2024-01-20T21:17:48.963+0000] {spark_submit.py:571} INFO - found org.slf4j#slf4j-api;1.7.26 in central
[2024-01-20T21:17:48.974+0000] {spark_submit.py:571} INFO - found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2024-01-20T21:17:48.989+0000] {spark_submit.py:571} INFO - found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2024-01-20T21:17:48.998+0000] {spark_submit.py:571} INFO - found org.reactivestreams#reactive-streams;1.0.3 in central
[2024-01-20T21:17:49.009+0000] {spark_submit.py:571} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2024-01-20T21:17:49.018+0000] {spark_submit.py:571} INFO - found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2024-01-20T21:17:49.031+0000] {spark_submit.py:571} INFO - found com.google.code.findbugs#jsr305;3.0.2 in central
[2024-01-20T21:17:49.044+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central
[2024-01-20T21:17:49.055+0000] {spark_submit.py:571} INFO - found com.datastax.oss#java-driver-query-builder;4.13.0 in central
[2024-01-20T21:17:49.068+0000] {spark_submit.py:571} INFO - found org.apache.commons#commons-lang3;3.10 in central
[2024-01-20T21:17:49.090+0000] {spark_submit.py:571} INFO - found com.thoughtworks.paranamer#paranamer;2.8 in central
[2024-01-20T21:17:49.096+0000] {spark_submit.py:571} INFO - found org.scala-lang#scala-reflect;2.12.11 in central
[2024-01-20T21:17:49.124+0000] {spark_submit.py:571} INFO - :: resolution report :: resolve 335ms :: artifacts dl 13ms
[2024-01-20T21:17:49.124+0000] {spark_submit.py:571} INFO - :: modules in use:
[2024-01-20T21:17:49.125+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]
[2024-01-20T21:17:49.125+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]
[2024-01-20T21:17:49.126+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]
[2024-01-20T21:17:49.127+0000] {spark_submit.py:571} INFO - com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2024-01-20T21:17:49.127+0000] {spark_submit.py:571} INFO - com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2024-01-20T21:17:49.128+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]
[2024-01-20T21:17:49.128+0000] {spark_submit.py:571} INFO - com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]
[2024-01-20T21:17:49.129+0000] {spark_submit.py:571} INFO - com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2024-01-20T21:17:49.130+0000] {spark_submit.py:571} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2024-01-20T21:17:49.130+0000] {spark_submit.py:571} INFO - com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2024-01-20T21:17:49.130+0000] {spark_submit.py:571} INFO - com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2024-01-20T21:17:49.131+0000] {spark_submit.py:571} INFO - com.typesafe#config;1.4.1 from central in [default]
[2024-01-20T21:17:49.131+0000] {spark_submit.py:571} INFO - io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2024-01-20T21:17:49.132+0000] {spark_submit.py:571} INFO - org.apache.commons#commons-lang3;3.10 from central in [default]
[2024-01-20T21:17:49.132+0000] {spark_submit.py:571} INFO - org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2024-01-20T21:17:49.133+0000] {spark_submit.py:571} INFO - org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2024-01-20T21:17:49.133+0000] {spark_submit.py:571} INFO - org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2024-01-20T21:17:49.134+0000] {spark_submit.py:571} INFO - org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]
[2024-01-20T21:17:49.134+0000] {spark_submit.py:571} INFO - org.slf4j#slf4j-api;1.7.26 from central in [default]
[2024-01-20T21:17:49.134+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:17:49.135+0000] {spark_submit.py:571} INFO - |                  |            modules            ||   artifacts   |
[2024-01-20T21:17:49.135+0000] {spark_submit.py:571} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-01-20T21:17:49.136+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:17:49.136+0000] {spark_submit.py:571} INFO - |      default     |   19  |   0   |   0   |   0   ||   19  |   0   |
[2024-01-20T21:17:49.137+0000] {spark_submit.py:571} INFO - ---------------------------------------------------------------------
[2024-01-20T21:17:49.137+0000] {spark_submit.py:571} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-49444278-b937-4e18-88b6-04caaae8cb5f
[2024-01-20T21:17:49.138+0000] {spark_submit.py:571} INFO - confs: [default]
[2024-01-20T21:17:49.142+0000] {spark_submit.py:571} INFO - 0 artifacts copied, 19 already retrieved (0kB/13ms)
[2024-01-20T21:17:49.278+0000] {spark_submit.py:571} INFO - 24/01/20 21:17:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-20T21:17:57.991+0000] {spark_submit.py:571} INFO - 2024-01-20 21:17:57.991354: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[2024-01-20T21:17:57.993+0000] {spark_submit.py:571} INFO - 2024-01-20 21:17:57.993574: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T21:17:58.020+0000] {spark_submit.py:571} INFO - 2024-01-20 21:17:58.020366: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[2024-01-20T21:17:58.021+0000] {spark_submit.py:571} INFO - 2024-01-20 21:17:58.020431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[2024-01-20T21:17:58.021+0000] {spark_submit.py:571} INFO - 2024-01-20 21:17:58.021572: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[2024-01-20T21:17:58.027+0000] {spark_submit.py:571} INFO - 2024-01-20 21:17:58.027065: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
[2024-01-20T21:17:58.028+0000] {spark_submit.py:571} INFO - 2024-01-20 21:17:58.027415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[2024-01-20T21:17:58.029+0000] {spark_submit.py:571} INFO - To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2024-01-20T21:17:58.820+0000] {spark_submit.py:571} INFO - 2024-01-20 21:17:58.820679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-01-20T21:18:00.604+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SparkContext: Running Spark version 3.4.1
[2024-01-20T21:18:00.620+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO ResourceUtils: ==============================================================
[2024-01-20T21:18:00.621+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-20T21:18:00.621+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO ResourceUtils: ==============================================================
[2024-01-20T21:18:00.621+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SparkContext: Submitted application: SparkDataBatch
[2024-01-20T21:18:00.638+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-20T21:18:00.650+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO ResourceProfile: Limiting resource is cpu
[2024-01-20T21:18:00.650+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-20T21:18:00.721+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SecurityManager: Changing view acls to: ***
[2024-01-20T21:18:00.722+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SecurityManager: Changing modify acls to: ***
[2024-01-20T21:18:00.722+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SecurityManager: Changing view acls groups to:
[2024-01-20T21:18:00.723+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SecurityManager: Changing modify acls groups to:
[2024-01-20T21:18:00.724+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-20T21:18:00.913+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO Utils: Successfully started service 'sparkDriver' on port 36155.
[2024-01-20T21:18:00.932+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SparkEnv: Registering MapOutputTracker
[2024-01-20T21:18:00.954+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-20T21:18:00.965+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-20T21:18:00.965+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-20T21:18:00.968+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-20T21:18:00.983+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5f2b62f1-f984-4ef0-95a1-5076bd335a29
[2024-01-20T21:18:00.994+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-20T21:18:01.005+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-20T21:18:01.091+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-20T21:18:01.136+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-20T21:18:01.161+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:36155/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.162+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:36155/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.163+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:36155/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.163+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:36155/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.163+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:36155/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.164+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:36155/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705785480597
[2024-01-20T21:18:01.164+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:36155/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705785480597
[2024-01-20T21:18:01.165+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:36155/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705785480597
[2024-01-20T21:18:01.165+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:36155/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.165+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:36155/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.166+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:36155/jars/com.typesafe_config-1.4.1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.166+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:36155/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705785480597
[2024-01-20T21:18:01.167+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:36155/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705785480597
[2024-01-20T21:18:01.167+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:36155/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705785480597
[2024-01-20T21:18:01.167+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:36155/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705785480597
[2024-01-20T21:18:01.168+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:36155/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.168+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:36155/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705785480597
[2024-01-20T21:18:01.168+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:36155/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705785480597
[2024-01-20T21:18:01.169+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:36155/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.169+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar at spark://45c5bfab8aa1:36155/files/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.169+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar
[2024-01-20T21:18:01.175+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar at spark://45c5bfab8aa1:36155/files/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.176+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar
[2024-01-20T21:18:01.179+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar at spark://45c5bfab8aa1:36155/files/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.180+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar
[2024-01-20T21:18:01.183+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar at spark://45c5bfab8aa1:36155/files/com.datastax.oss_java-driver-core-shaded-4.13.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.184+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.datastax.oss_java-driver-core-shaded-4.13.0.jar
[2024-01-20T21:18:01.192+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar at spark://45c5bfab8aa1:36155/files/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.193+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar
[2024-01-20T21:18:01.196+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://45c5bfab8aa1:36155/files/org.apache.commons_commons-lang3-3.10.jar with timestamp 1705785480597
[2024-01-20T21:18:01.196+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/org.apache.commons_commons-lang3-3.10.jar
[2024-01-20T21:18:01.200+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://45c5bfab8aa1:36155/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1705785480597
[2024-01-20T21:18:01.201+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.thoughtworks.paranamer_paranamer-2.8.jar
[2024-01-20T21:18:01.204+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://45c5bfab8aa1:36155/files/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1705785480597
[2024-01-20T21:18:01.204+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/org.scala-lang_scala-reflect-2.12.11.jar
[2024-01-20T21:18:01.210+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://45c5bfab8aa1:36155/files/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.210+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.datastax.oss_native-protocol-1.5.0.jar
[2024-01-20T21:18:01.214+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://45c5bfab8aa1:36155/files/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.215+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2024-01-20T21:18:01.221+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://45c5bfab8aa1:36155/files/com.typesafe_config-1.4.1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.222+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.typesafe_config-1.4.1.jar
[2024-01-20T21:18:01.225+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://45c5bfab8aa1:36155/files/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1705785480597
[2024-01-20T21:18:01.225+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/org.slf4j_slf4j-api-1.7.26.jar
[2024-01-20T21:18:01.229+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://45c5bfab8aa1:36155/files/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1705785480597
[2024-01-20T21:18:01.229+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2024-01-20T21:18:01.232+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://45c5bfab8aa1:36155/files/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1705785480597
[2024-01-20T21:18:01.232+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2024-01-20T21:18:01.235+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://45c5bfab8aa1:36155/files/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1705785480597
[2024-01-20T21:18:01.236+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/org.reactivestreams_reactive-streams-1.0.3.jar
[2024-01-20T21:18:01.238+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://45c5bfab8aa1:36155/files/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1705785480597
[2024-01-20T21:18:01.239+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2024-01-20T21:18:01.242+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://45c5bfab8aa1:36155/files/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1705785480597
[2024-01-20T21:18:01.242+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2024-01-20T21:18:01.245+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://45c5bfab8aa1:36155/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1705785480597
[2024-01-20T21:18:01.245+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.google.code.findbugs_jsr305-3.0.2.jar
[2024-01-20T21:18:01.248+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar at spark://45c5bfab8aa1:36155/files/com.datastax.oss_java-driver-query-builder-4.13.0.jar with timestamp 1705785480597
[2024-01-20T21:18:01.248+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar to /tmp/spark-edfa152e-8549-4f2c-89af-6f4bee3ee828/userFiles-cb0b859d-3d58-49d0-b55b-eff077346cea/com.datastax.oss_java-driver-query-builder-4.13.0.jar
[2024-01-20T21:18:01.325+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://172.19.0.5:7077...
[2024-01-20T21:18:01.364+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO TransportClientFactory: Successfully created connection to /172.19.0.5:7077 after 25 ms (0 ms spent in bootstraps)
[2024-01-20T21:18:01.452+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240120211801-0030
[2024-01-20T21:18:01.453+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240120211801-0030/0 on worker-20240120190212-172.19.0.7-41203 (172.19.0.7:41203) with 2 core(s)
[2024-01-20T21:18:01.455+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20240120211801-0030/0 on hostPort 172.19.0.7:41203 with 2 core(s), 1024.0 MiB RAM
[2024-01-20T21:18:01.459+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42447.
[2024-01-20T21:18:01.460+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO NettyBlockTransferService: Server created on 45c5bfab8aa1:42447
[2024-01-20T21:18:01.461+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-20T21:18:01.468+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45c5bfab8aa1, 42447, None)
[2024-01-20T21:18:01.471+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO BlockManagerMasterEndpoint: Registering block manager 45c5bfab8aa1:42447 with 434.4 MiB RAM, BlockManagerId(driver, 45c5bfab8aa1, 42447, None)
[2024-01-20T21:18:01.474+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45c5bfab8aa1, 42447, None)
[2024-01-20T21:18:01.475+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45c5bfab8aa1, 42447, None)
[2024-01-20T21:18:01.578+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240120211801-0030/0 is now RUNNING
[2024-01-20T21:18:01.700+0000] {spark_submit.py:571} INFO - 24/01/20 21:18:01 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-20T21:18:01.951+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)
[2024-01-20T21:18:01.955+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T21:18:01.957+0000] {spark_submit.py:571} INFO - WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172.19.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version
[2024-01-20T21:18:02.114+0000] {spark_submit.py:571} INFO - Keyspace created successfully!
[2024-01-20T21:18:02.116+0000] {spark_submit.py:571} INFO - Table created successfully!
[2024-01-20T21:18:02.116+0000] {spark_submit.py:571} INFO - Charger les donn√©es historiques depuis Cassandra
[2024-01-20T21:18:02.117+0000] {spark_submit.py:571} INFO - start_of_day_str : 2024-01-19 00:00:00
[2024-01-20T21:18:02.117+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes
[2024-01-20T21:18:02.118+0000] {spark_submit.py:571} INFO - first_timestamp_result : Row(close_price=42100.0)
[2024-01-20T21:18:02.121+0000] {spark_submit.py:571} INFO - last_timestamp_result : Row(bitcoin_price=41800.5678)
[2024-01-20T21:18:02.125+0000] {spark_submit.py:571} INFO - sum_posts_result : Row(system_sum_posts_last_5_minutes=554)
[2024-01-20T21:18:02.125+0000] {spark_submit.py:571} INFO - Obtenez les valeurs n√©cessaires
[2024-01-20T21:18:02.126+0000] {spark_submit.py:571} INFO - first_timestamp_bitcoin_price :  42100.0
[2024-01-20T21:18:02.126+0000] {spark_submit.py:571} INFO - last_timestamp_bitcoin_price :  41800.5678
[2024-01-20T21:18:02.127+0000] {spark_submit.py:571} INFO - sum_posts_last_day :  554
[2024-01-20T21:18:02.127+0000] {spark_submit.py:571} INFO - change_from_kafka : -0.7112403800475129
[2024-01-20T21:18:02.127+0000] {spark_submit.py:571} INFO - Ex√©cutez les requ√™tes ...
[2024-01-20T21:18:02.128+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish_result :  Row(system_avg_avg_sentiment_bullish=0.49302727272727276)
[2024-01-20T21:18:02.133+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish_result :  Row(system_avg_avg_sentiment_bearish=0.44849090909090905)
[2024-01-20T21:18:02.137+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral_result :  Row(system_avg_avg_sentiment_neutral=0.5075363636363637)
[2024-01-20T21:18:02.138+0000] {spark_submit.py:571} INFO - # Obtenez les valeurs moyennes n√©cessaires
[2024-01-20T21:18:02.139+0000] {spark_submit.py:571} INFO - avg_sentiment_bullish :  0.49302727272727276
[2024-01-20T21:18:02.139+0000] {spark_submit.py:571} INFO - avg_sentiment_bearish :  0.44849090909090905
[2024-01-20T21:18:02.140+0000] {spark_submit.py:571} INFO - avg_sentiment_neutral :  0.5075363636363637
[2024-01-20T21:18:02.140+0000] {spark_submit.py:571} INFO - # Ex√©cutez les requ√™tes...
[2024-01-20T21:18:02.155+0000] {spark_submit.py:571} INFO - max_price :  42500.7891
[2024-01-20T21:18:02.156+0000] {spark_submit.py:571} INFO - min_price :  41500.6543
[2024-01-20T21:18:02.156+0000] {spark_submit.py:571} INFO - model_path :  /opt/***/shared_volume/best_model_sen.h5
[2024-01-20T21:18:02.161+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.2.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
[2024-01-20T21:18:02.162+0000] {spark_submit.py:571} INFO - https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
[2024-01-20T21:18:02.163+0000] {spark_submit.py:571} INFO - warnings.warn(
[2024-01-20T21:18:02.163+0000] {spark_submit.py:571} INFO - load model ...
[2024-01-20T21:18:02.891+0000] {spark_submit.py:571} INFO - apply MinMaxScaler ..
[2024-01-20T21:18:02.892+0000] {spark_submit.py:571} INFO - scaled_bitcoin_prices : [[0.75874094]
[2024-01-20T21:18:02.893+0000] {spark_submit.py:571} INFO - [0.75416341]
[2024-01-20T21:18:02.893+0000] {spark_submit.py:571} INFO - [0.77705107]
[2024-01-20T21:18:02.893+0000] {spark_submit.py:571} INFO - [0.78162861]
[2024-01-20T21:18:02.893+0000] {spark_submit.py:571} INFO - [0.76334447]]
[2024-01-20T21:18:02.894+0000] {spark_submit.py:571} INFO - input data :  [[[0.75874094 0.6        0.3        0.1       ]
[2024-01-20T21:18:02.894+0000] {spark_submit.py:571} INFO - [0.75416341 0.7        0.1        0.2       ]
[2024-01-20T21:18:02.894+0000] {spark_submit.py:571} INFO - [0.77705107 0.5        0.4        0.1       ]
[2024-01-20T21:18:02.895+0000] {spark_submit.py:571} INFO - [0.78162861 0.8        0.2        0.1       ]
[2024-01-20T21:18:02.895+0000] {spark_submit.py:571} INFO - [0.76334447 0.49302727 0.44849091 0.50753636]]]
[2024-01-20T21:18:03.731+0000] {spark_submit.py:571} INFO - 
[2024-01-20T21:18:03.732+0000] {spark_submit.py:571} INFO - 1/1 [==============================] - ETA: 0s
[2024-01-20T21:18:03.733+0000] {spark_submit.py:571} INFO - 1/1 [==============================] - 1s 793ms/step
[2024-01-20T21:18:03.734+0000] {spark_submit.py:571} INFO - Predicted Close Price for the Next Day: 32224.2
[2024-01-20T21:18:04.523+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=BatchLayer, task_id=submit_spark_job, execution_date=20240118T235900, start_date=20240120T211747, end_date=20240120T211804
[2024-01-20T21:18:04.575+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-01-20T21:18:04.587+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
