[2024-01-17T01:55:01.642+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T01:55:01.648+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T01:55:01.648+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2024-01-17T01:55:01.658+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T01:55:01.660+0000] {standard_task_runner.py:57} INFO - Started process 727 to run task
[2024-01-17T01:55:01.668+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '58', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpok_gk0s8']
[2024-01-17T01:55:01.670+0000] {standard_task_runner.py:85} INFO - Job 58: Subtask spark_submit_task
[2024-01-17T01:55:01.739+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host a81a9f2a1c09
[2024-01-17T01:55:01.807+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T01:55:01.808+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-01-17T01:55:01.809+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker run --rm -v /opt/***/dags:/opt/***/dags bitnami/spark:latest bin/spark-submit /opt/***/dags/spark_stream.py']
[2024-01-17T01:55:01.813+0000] {subprocess.py:86} INFO - Output:
[2024-01-17T01:55:01.847+0000] {subprocess.py:93} INFO - docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.
[2024-01-17T01:55:01.847+0000] {subprocess.py:93} INFO - See 'docker run --help'.
[2024-01-17T01:55:01.849+0000] {subprocess.py:97} INFO - Command exited with return code 125
[2024-01-17T01:55:01.860+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 125.
[2024-01-17T01:55:01.864+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T015501, end_date=20240117T015501
[2024-01-17T01:55:01.879+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 58 for task spark_submit_task (Bash command failed. The command returned a non-zero exit code 125.; 727)
[2024-01-17T01:55:01.919+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T01:55:01.936+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-17T02:38:11.857+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T02:38:11.863+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T02:38:11.863+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2024-01-17T02:38:11.873+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T02:38:11.876+0000] {standard_task_runner.py:57} INFO - Started process 527 to run task
[2024-01-17T02:38:11.879+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpzcb915_h']
[2024-01-17T02:38:11.880+0000] {standard_task_runner.py:85} INFO - Job 50: Subtask spark_submit_task
[2024-01-17T02:38:11.926+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host 7f29d30c8362
[2024-01-17T02:38:11.988+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T02:38:11.996+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-17T02:38:11.997+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark-master:7077 --name spark_submit_job --verbose --queue default --deploy-mode client /opt/bitnami/scripts/spark_stream.py
[2024-01-17T02:38:12.167+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-17T02:38:12.168+0000] {spark_submit.py:571} INFO - JAVA_HOME is not set
[2024-01-17T02:38:12.176+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark-master:7077 --name spark_submit_job --verbose --queue default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 1.
[2024-01-17T02:38:12.179+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T023811, end_date=20240117T023812
[2024-01-17T02:38:12.190+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 50 for task spark_submit_task (Cannot execute: spark-submit --master spark-master:7077 --name spark_submit_job --verbose --queue default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 1.; 527)
[2024-01-17T02:38:12.212+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T02:38:12.230+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-17T03:27:17.603+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T03:27:17.608+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T03:27:17.609+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2024-01-17T03:27:17.619+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T03:27:17.621+0000] {standard_task_runner.py:57} INFO - Started process 552 to run task
[2024-01-17T03:27:17.623+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpj1b7r42d']
[2024-01-17T03:27:17.624+0000] {standard_task_runner.py:85} INFO - Job 50: Subtask spark_submit_task
[2024-01-17T03:27:17.663+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host 776833dc279e
[2024-01-17T03:27:17.721+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T03:27:17.728+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-17T03:27:17.728+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py
[2024-01-17T03:27:17.824+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-17T03:27:17.825+0000] {spark_submit.py:571} INFO - JAVA_HOME is not set
[2024-01-17T03:27:17.833+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 1.
[2024-01-17T03:27:17.835+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T032717, end_date=20240117T032717
[2024-01-17T03:27:17.846+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 50 for task spark_submit_task (Cannot execute: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 1.; 552)
[2024-01-17T03:27:17.876+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T03:27:17.892+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-17T03:31:04.595+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T03:31:04.601+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T03:31:04.602+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2024-01-17T03:31:04.614+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T03:31:04.616+0000] {standard_task_runner.py:57} INFO - Started process 1035 to run task
[2024-01-17T03:31:04.619+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '99', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpg_e9sajf']
[2024-01-17T03:31:04.620+0000] {standard_task_runner.py:85} INFO - Job 99: Subtask spark_submit_task
[2024-01-17T03:31:04.665+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host 776833dc279e
[2024-01-17T03:31:04.733+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T03:31:04.741+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-17T03:31:04.742+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py
[2024-01-17T03:31:04.847+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-17T03:31:04.847+0000] {spark_submit.py:571} INFO - JAVA_HOME is not set
[2024-01-17T03:31:04.856+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 1.
[2024-01-17T03:31:04.858+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T033104, end_date=20240117T033104
[2024-01-17T03:31:04.867+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 99 for task spark_submit_task (Cannot execute: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 1.; 1035)
[2024-01-17T03:31:04.871+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T03:31:04.887+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-17T03:36:24.484+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T03:36:24.488+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T03:36:24.489+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2024-01-17T03:36:24.498+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T03:36:24.500+0000] {standard_task_runner.py:57} INFO - Started process 241 to run task
[2024-01-17T03:36:24.502+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '148', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpwswlbx9p']
[2024-01-17T03:36:24.503+0000] {standard_task_runner.py:85} INFO - Job 148: Subtask spark_submit_task
[2024-01-17T03:36:24.540+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host 776833dc279e
[2024-01-17T03:36:24.598+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T03:36:24.606+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-17T03:36:24.607+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py
[2024-01-17T03:36:24.706+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-17T03:36:24.707+0000] {spark_submit.py:571} INFO - JAVA_HOME is not set
[2024-01-17T03:36:24.715+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 1.
[2024-01-17T03:36:24.717+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T033624, end_date=20240117T033624
[2024-01-17T03:36:24.728+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 148 for task spark_submit_task (Cannot execute: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 1.; 241)
[2024-01-17T03:36:24.755+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T03:36:24.772+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-17T03:53:25.302+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T03:53:25.309+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T03:53:25.309+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2024-01-17T03:53:25.320+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T03:53:25.324+0000] {standard_task_runner.py:57} INFO - Started process 6542 to run task
[2024-01-17T03:53:25.326+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '251', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpx1nlu1ao']
[2024-01-17T03:53:25.327+0000] {standard_task_runner.py:85} INFO - Job 251: Subtask spark_submit_task
[2024-01-17T03:53:25.369+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host 776833dc279e
[2024-01-17T03:53:25.438+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T03:53:25.446+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-17T03:53:25.447+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py
[2024-01-17T03:53:25.597+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-17T03:53:26.908+0000] {spark_submit.py:571} INFO - Using properties file: null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - Parsed arguments:
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - master                  spark://192.168.192.5:7077
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - remote                  null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - deployMode              client
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - executorMemory          null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - executorCores           null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - totalExecutorCores      null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - propertiesFile          null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - driverMemory            null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - driverCores             null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - driverExtraClassPath    null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - driverExtraLibraryPath  null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - driverExtraJavaOptions  null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - supervise               false
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - queue                   root.default
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - numExecutors            null
[2024-01-17T03:53:27.024+0000] {spark_submit.py:571} INFO - files                   null
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - pyFiles                 null
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - archives                null
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - mainClass               null
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - primaryResource         file:/opt/bitnami/scripts/spark_stream.py
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - name                    spark_submit_job
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - childArgs               []
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - jars                    null
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - packages                null
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - packagesExclusions      null
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - repositories            null
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - verbose                 true
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - 
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - Spark properties used, including those specified through
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - --conf and those from the properties file null:
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - 
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - 
[2024-01-17T03:53:27.025+0000] {spark_submit.py:571} INFO - 
[2024-01-17T03:53:27.282+0000] {spark_submit.py:571} INFO - Main class:
[2024-01-17T03:53:27.282+0000] {spark_submit.py:571} INFO - org.apache.spark.deploy.PythonRunner
[2024-01-17T03:53:27.282+0000] {spark_submit.py:571} INFO - Arguments:
[2024-01-17T03:53:27.283+0000] {spark_submit.py:571} INFO - file:/opt/bitnami/scripts/spark_stream.py
[2024-01-17T03:53:27.283+0000] {spark_submit.py:571} INFO - null
[2024-01-17T03:53:27.286+0000] {spark_submit.py:571} INFO - Spark config:
[2024-01-17T03:53:27.286+0000] {spark_submit.py:571} INFO - (spark.app.name,spark_submit_job)
[2024-01-17T03:53:27.286+0000] {spark_submit.py:571} INFO - (spark.app.submitTime,1705463607265)
[2024-01-17T03:53:27.286+0000] {spark_submit.py:571} INFO - (spark.master,spark://192.168.192.5:7077)
[2024-01-17T03:53:27.286+0000] {spark_submit.py:571} INFO - (spark.submit.deployMode,client)
[2024-01-17T03:53:27.286+0000] {spark_submit.py:571} INFO - (spark.submit.pyFiles,)
[2024-01-17T03:53:27.287+0000] {spark_submit.py:571} INFO - Classpath elements:
[2024-01-17T03:53:27.287+0000] {spark_submit.py:571} INFO - 
[2024-01-17T03:53:27.287+0000] {spark_submit.py:571} INFO - 
[2024-01-17T03:53:27.287+0000] {spark_submit.py:571} INFO - 
[2024-01-17T03:53:27.483+0000] {spark_submit.py:571} INFO - python3: can't open file '/opt/bitnami/scripts/spark_stream.py': [Errno 2] No such file or directory
[2024-01-17T03:53:27.499+0000] {spark_submit.py:571} INFO - 24/01/17 03:53:27 INFO ShutdownHookManager: Shutdown hook called
[2024-01-17T03:53:27.502+0000] {spark_submit.py:571} INFO - 24/01/17 03:53:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-8519393b-cd70-4439-b909-18aafaece610
[2024-01-17T03:53:27.541+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 2.
[2024-01-17T03:53:27.545+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T035325, end_date=20240117T035327
[2024-01-17T03:53:27.560+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 251 for task spark_submit_task (Cannot execute: spark-submit --master spark://192.168.192.5:7077 --name spark_submit_job --verbose --queue root.default --deploy-mode client /opt/bitnami/scripts/spark_stream.py. Error code is: 2.; 6542)
[2024-01-17T03:53:27.586+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T03:53:27.609+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-17T04:39:29.755+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T04:39:29.762+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T04:39:29.762+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2024-01-17T04:39:29.773+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T04:39:29.775+0000] {standard_task_runner.py:57} INFO - Started process 7370 to run task
[2024-01-17T04:39:29.778+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '97', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpzk6s7ns0']
[2024-01-17T04:39:29.779+0000] {standard_task_runner.py:85} INFO - Job 97: Subtask spark_submit_task
[2024-01-17T04:39:29.819+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host 614a419ad930
[2024-01-17T04:39:29.880+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T04:39:29.887+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-17T04:39:29.888+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/bitnami/code/spark_stream.py
[2024-01-17T04:39:29.989+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-17T04:39:30.986+0000] {spark_submit.py:571} INFO - Using properties file: null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - Parsed arguments:
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - master                  spark://192.168.224.5:7077
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - remote                  null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - deployMode              client
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - executorMemory          null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - executorCores           null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - totalExecutorCores      null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - propertiesFile          null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - driverMemory            null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - driverCores             null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - driverExtraClassPath    null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - driverExtraLibraryPath  null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - driverExtraJavaOptions  null
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - supervise               false
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - queue                   root.default
[2024-01-17T04:39:31.064+0000] {spark_submit.py:571} INFO - numExecutors            null
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - files                   null
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - pyFiles                 null
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - archives                null
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - mainClass               org.apache.spark.deploy.PythonRunner
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - primaryResource         file:/opt/bitnami/code/spark_stream.py
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - name                    spark_submit_job
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - childArgs               []
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - jars                    null
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - packages                null
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - packagesExclusions      null
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - repositories            null
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - verbose                 true
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - Spark properties used, including those specified through
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - --conf and those from the properties file null:
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:39:31.065+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:39:31.229+0000] {spark_submit.py:571} INFO - Main class:
[2024-01-17T04:39:31.229+0000] {spark_submit.py:571} INFO - org.apache.spark.deploy.PythonRunner
[2024-01-17T04:39:31.229+0000] {spark_submit.py:571} INFO - Arguments:
[2024-01-17T04:39:31.229+0000] {spark_submit.py:571} INFO - file:/opt/bitnami/code/spark_stream.py
[2024-01-17T04:39:31.229+0000] {spark_submit.py:571} INFO - null
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - Spark config:
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - (spark.app.name,spark_submit_job)
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - (spark.app.submitTime,1705466371218)
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - (spark.master,spark://192.168.224.5:7077)
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - (spark.submit.deployMode,client)
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - (spark.submit.pyFiles,)
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - Classpath elements:
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:39:31.232+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:39:31.365+0000] {spark_submit.py:571} INFO - python3: can't open file '/opt/bitnami/code/spark_stream.py': [Errno 2] No such file or directory
[2024-01-17T04:39:31.378+0000] {spark_submit.py:571} INFO - 24/01/17 04:39:31 INFO ShutdownHookManager: Shutdown hook called
[2024-01-17T04:39:31.379+0000] {spark_submit.py:571} INFO - 24/01/17 04:39:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-fef3c37a-b19c-4179-aa19-94a68557d42b
[2024-01-17T04:39:31.412+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/bitnami/code/spark_stream.py. Error code is: 2.
[2024-01-17T04:39:31.414+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T043929, end_date=20240117T043931
[2024-01-17T04:39:31.424+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 97 for task spark_submit_task (Cannot execute: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/bitnami/code/spark_stream.py. Error code is: 2.; 7370)
[2024-01-17T04:39:31.437+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T04:39:31.452+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-17T04:47:02.846+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T04:47:02.852+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T04:47:02.852+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-17T04:47:02.863+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T04:47:02.867+0000] {standard_task_runner.py:57} INFO - Started process 15021 to run task
[2024-01-17T04:47:02.869+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '156', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpbcij5ed8']
[2024-01-17T04:47:02.870+0000] {standard_task_runner.py:85} INFO - Job 156: Subtask spark_submit_task
[2024-01-17T04:47:02.911+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host 614a419ad930
[2024-01-17T04:47:02.970+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T04:47:02.978+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-17T04:47:02.979+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/***/dags/spark_stream.py
[2024-01-17T04:47:03.079+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-17T04:47:04.157+0000] {spark_submit.py:571} INFO - Using properties file: null
[2024-01-17T04:47:04.238+0000] {spark_submit.py:571} INFO - Parsed arguments:
[2024-01-17T04:47:04.238+0000] {spark_submit.py:571} INFO - master                  spark://192.168.224.5:7077
[2024-01-17T04:47:04.238+0000] {spark_submit.py:571} INFO - remote                  null
[2024-01-17T04:47:04.238+0000] {spark_submit.py:571} INFO - deployMode              client
[2024-01-17T04:47:04.238+0000] {spark_submit.py:571} INFO - executorMemory          null
[2024-01-17T04:47:04.238+0000] {spark_submit.py:571} INFO - executorCores           null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - totalExecutorCores      null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - propertiesFile          null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - driverMemory            null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - driverCores             null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - driverExtraClassPath    null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - driverExtraLibraryPath  null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - driverExtraJavaOptions  null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - supervise               false
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - queue                   root.default
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - numExecutors            null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - files                   null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - pyFiles                 null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - archives                null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - mainClass               org.apache.spark.deploy.PythonRunner
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - primaryResource         file:/opt/***/dags/spark_stream.py
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - name                    spark_submit_job
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - childArgs               []
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - jars                    null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - packages                null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - packagesExclusions      null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - repositories            null
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - verbose                 true
[2024-01-17T04:47:04.239+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:47:04.240+0000] {spark_submit.py:571} INFO - Spark properties used, including those specified through
[2024-01-17T04:47:04.240+0000] {spark_submit.py:571} INFO - --conf and those from the properties file null:
[2024-01-17T04:47:04.240+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:47:04.240+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:47:04.240+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:47:04.420+0000] {spark_submit.py:571} INFO - Main class:
[2024-01-17T04:47:04.420+0000] {spark_submit.py:571} INFO - org.apache.spark.deploy.PythonRunner
[2024-01-17T04:47:04.420+0000] {spark_submit.py:571} INFO - Arguments:
[2024-01-17T04:47:04.420+0000] {spark_submit.py:571} INFO - file:/opt/***/dags/spark_stream.py
[2024-01-17T04:47:04.421+0000] {spark_submit.py:571} INFO - null
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - Spark config:
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - (spark.app.name,spark_submit_job)
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - (spark.app.submitTime,1705466824408)
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - (spark.master,spark://192.168.224.5:7077)
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - (spark.submit.deployMode,client)
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - (spark.submit.pyFiles,)
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - Classpath elements:
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:47:04.423+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:47:04.424+0000] {spark_submit.py:571} INFO - 
[2024-01-17T04:47:05.889+0000] {spark_submit.py:571} INFO - None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[2024-01-17T04:47:06.079+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SparkContext: Running Spark version 3.5.0
[2024-01-17T04:47:06.081+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SparkContext: OS info Linux, 5.10.16.3-microsoft-standard-WSL2, amd64
[2024-01-17T04:47:06.082+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SparkContext: Java version 11.0.21
[2024-01-17T04:47:06.139+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-17T04:47:06.221+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO ResourceUtils: ==============================================================
[2024-01-17T04:47:06.221+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-17T04:47:06.221+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO ResourceUtils: ==============================================================
[2024-01-17T04:47:06.221+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SparkContext: Submitted application: SparkDataStreaming
[2024-01-17T04:47:06.240+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-17T04:47:06.251+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO ResourceProfile: Limiting resource is cpu
[2024-01-17T04:47:06.252+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-17T04:47:06.304+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SecurityManager: Changing view acls to: ***
[2024-01-17T04:47:06.304+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SecurityManager: Changing modify acls to: ***
[2024-01-17T04:47:06.304+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SecurityManager: Changing view acls groups to:
[2024-01-17T04:47:06.305+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SecurityManager: Changing modify acls groups to:
[2024-01-17T04:47:06.305+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-17T04:47:06.501+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO Utils: Successfully started service 'sparkDriver' on port 41507.
[2024-01-17T04:47:06.528+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SparkEnv: Registering MapOutputTracker
[2024-01-17T04:47:06.565+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-17T04:47:06.584+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-17T04:47:06.584+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-17T04:47:06.589+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-17T04:47:06.609+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f4387e5b-3608-4c96-9f4f-9b642f20cefa
[2024-01-17T04:47:06.623+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-17T04:47:06.639+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-17T04:47:06.800+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-17T04:47:06.866+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-17T04:47:06.980+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:06 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://192.168.224.5:7077...
[2024-01-17T04:47:07.024+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO TransportClientFactory: Successfully created connection to /192.168.224.5:7077 after 26 ms (0 ms spent in bootstraps)
[2024-01-17T04:47:07.103+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240117044707-0030
[2024-01-17T04:47:07.105+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240117044707-0030/0 on worker-20240117040815-192.168.224.7-45465 (192.168.224.7:45465) with 2 core(s)
[2024-01-17T04:47:07.107+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20240117044707-0030/0 on hostPort 192.168.224.7:45465 with 2 core(s), 1024.0 MiB RAM
[2024-01-17T04:47:07.114+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42453.
[2024-01-17T04:47:07.115+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO NettyBlockTransferService: Server created on 614a419ad930:42453
[2024-01-17T04:47:07.116+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-17T04:47:07.127+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 614a419ad930, 42453, None)
[2024-01-17T04:47:07.131+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO BlockManagerMasterEndpoint: Registering block manager 614a419ad930:42453 with 434.4 MiB RAM, BlockManagerId(driver, 614a419ad930, 42453, None)
[2024-01-17T04:47:07.135+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 614a419ad930, 42453, None)
[2024-01-17T04:47:07.136+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240117044707-0030/0 is now RUNNING
[2024-01-17T04:47:07.137+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 614a419ad930, 42453, None)
[2024-01-17T04:47:07.329+0000] {spark_submit.py:571} INFO - 24/01/17 04:47:07 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-17T04:47:08.621+0000] {spark_submit.py:571} INFO - WARNING:root:kafka dataframe could not be created because: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[2024-01-17T04:47:08.627+0000] {spark_submit.py:571} INFO - ERROR:root:An error occurred in the Spark Streaming job: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[2024-01-17T04:47:08.877+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-17T04:47:08.877+0000] {spark_submit.py:571} INFO - File "/opt/***/dags/spark_stream.py", line 331, in <module>
[2024-01-17T04:47:08.877+0000] {spark_submit.py:571} INFO - main()
[2024-01-17T04:47:08.877+0000] {spark_submit.py:571} INFO - File "/opt/***/dags/spark_stream.py", line 328, in main
[2024-01-17T04:47:08.878+0000] {spark_submit.py:571} INFO - cassandra_session.shutdown()
[2024-01-17T04:47:08.878+0000] {spark_submit.py:571} INFO - UnboundLocalError: local variable 'cassandra_session' referenced before assignment
[2024-01-17T04:47:09.031+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/***/dags/spark_stream.py. Error code is: 1.
[2024-01-17T04:47:09.034+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T044702, end_date=20240117T044709
[2024-01-17T04:47:09.043+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 156 for task spark_submit_task (Cannot execute: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/***/dags/spark_stream.py. Error code is: 1.; 15021)
[2024-01-17T04:47:09.077+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T04:47:09.092+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-01-17T05:01:38.024+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T05:01:38.033+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [queued]>
[2024-01-17T05:01:38.034+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-01-17T05:01:38.050+0000] {taskinstance.py:1382} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-01-17 01:50:00+00:00
[2024-01-17T05:01:38.053+0000] {standard_task_runner.py:57} INFO - Started process 27605 to run task
[2024-01-17T05:01:38.057+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'spark_stream_job', 'spark_submit_task', 'scheduled__2024-01-17T01:50:00+00:00', '--job-id', '280', '--raw', '--subdir', 'DAGS_FOLDER/spark_streaming_dag.py', '--cfg-path', '/tmp/tmpi0hyi2bm']
[2024-01-17T05:01:38.058+0000] {standard_task_runner.py:85} INFO - Job 280: Subtask spark_submit_task
[2024-01-17T05:01:38.106+0000] {task_command.py:415} INFO - Running <TaskInstance: spark_stream_job.spark_submit_task scheduled__2024-01-17T01:50:00+00:00 [running]> on host 614a419ad930
[2024-01-17T05:01:38.195+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='me' AIRFLOW_CTX_DAG_ID='spark_stream_job' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-01-17T01:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-17T01:50:00+00:00'
[2024-01-17T05:01:38.206+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-17T05:01:38.207+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/***/dags/spark_stream.py
[2024-01-17T05:01:38.318+0000] {spark_submit.py:571} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-01-17T05:01:39.565+0000] {spark_submit.py:571} INFO - Using properties file: null
[2024-01-17T05:01:39.654+0000] {spark_submit.py:571} INFO - Parsed arguments:
[2024-01-17T05:01:39.654+0000] {spark_submit.py:571} INFO - master                  spark://192.168.224.5:7077
[2024-01-17T05:01:39.654+0000] {spark_submit.py:571} INFO - remote                  null
[2024-01-17T05:01:39.654+0000] {spark_submit.py:571} INFO - deployMode              client
[2024-01-17T05:01:39.654+0000] {spark_submit.py:571} INFO - executorMemory          null
[2024-01-17T05:01:39.654+0000] {spark_submit.py:571} INFO - executorCores           null
[2024-01-17T05:01:39.654+0000] {spark_submit.py:571} INFO - totalExecutorCores      null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - propertiesFile          null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - driverMemory            null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - driverCores             null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - driverExtraClassPath    null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - driverExtraLibraryPath  null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - driverExtraJavaOptions  null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - supervise               false
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - queue                   root.default
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - numExecutors            null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - files                   null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - pyFiles                 null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - archives                null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - mainClass               org.apache.spark.deploy.PythonRunner
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - primaryResource         file:/opt/***/dags/spark_stream.py
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - name                    spark_submit_job
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - childArgs               []
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - jars                    null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - packages                null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - packagesExclusions      null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - repositories            null
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - verbose                 true
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - 
[2024-01-17T05:01:39.655+0000] {spark_submit.py:571} INFO - Spark properties used, including those specified through
[2024-01-17T05:01:39.656+0000] {spark_submit.py:571} INFO - --conf and those from the properties file null:
[2024-01-17T05:01:39.656+0000] {spark_submit.py:571} INFO - 
[2024-01-17T05:01:39.656+0000] {spark_submit.py:571} INFO - 
[2024-01-17T05:01:39.656+0000] {spark_submit.py:571} INFO - 
[2024-01-17T05:01:39.830+0000] {spark_submit.py:571} INFO - Main class:
[2024-01-17T05:01:39.830+0000] {spark_submit.py:571} INFO - org.apache.spark.deploy.PythonRunner
[2024-01-17T05:01:39.830+0000] {spark_submit.py:571} INFO - Arguments:
[2024-01-17T05:01:39.830+0000] {spark_submit.py:571} INFO - file:/opt/***/dags/spark_stream.py
[2024-01-17T05:01:39.830+0000] {spark_submit.py:571} INFO - null
[2024-01-17T05:01:39.833+0000] {spark_submit.py:571} INFO - Spark config:
[2024-01-17T05:01:39.833+0000] {spark_submit.py:571} INFO - (spark.app.name,spark_submit_job)
[2024-01-17T05:01:39.833+0000] {spark_submit.py:571} INFO - (spark.app.submitTime,1705467699817)
[2024-01-17T05:01:39.833+0000] {spark_submit.py:571} INFO - (spark.master,spark://192.168.224.5:7077)
[2024-01-17T05:01:39.833+0000] {spark_submit.py:571} INFO - (spark.submit.deployMode,client)
[2024-01-17T05:01:39.833+0000] {spark_submit.py:571} INFO - (spark.submit.pyFiles,)
[2024-01-17T05:01:39.833+0000] {spark_submit.py:571} INFO - Classpath elements:
[2024-01-17T05:01:39.834+0000] {spark_submit.py:571} INFO - 
[2024-01-17T05:01:39.834+0000] {spark_submit.py:571} INFO - 
[2024-01-17T05:01:39.834+0000] {spark_submit.py:571} INFO - 
[2024-01-17T05:01:41.379+0000] {spark_submit.py:571} INFO - None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
[2024-01-17T05:01:41.542+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SparkContext: Running Spark version 3.5.0
[2024-01-17T05:01:41.543+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SparkContext: OS info Linux, 5.10.16.3-microsoft-standard-WSL2, amd64
[2024-01-17T05:01:41.544+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SparkContext: Java version 11.0.21
[2024-01-17T05:01:41.590+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-17T05:01:41.651+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO ResourceUtils: ==============================================================
[2024-01-17T05:01:41.652+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-17T05:01:41.652+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO ResourceUtils: ==============================================================
[2024-01-17T05:01:41.652+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SparkContext: Submitted application: SparkDataStreaming
[2024-01-17T05:01:41.667+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-17T05:01:41.677+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO ResourceProfile: Limiting resource is cpu
[2024-01-17T05:01:41.677+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-17T05:01:41.715+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SecurityManager: Changing view acls to: ***
[2024-01-17T05:01:41.715+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SecurityManager: Changing modify acls to: ***
[2024-01-17T05:01:41.715+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SecurityManager: Changing view acls groups to:
[2024-01-17T05:01:41.715+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SecurityManager: Changing modify acls groups to:
[2024-01-17T05:01:41.715+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-01-17T05:01:41.883+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO Utils: Successfully started service 'sparkDriver' on port 32921.
[2024-01-17T05:01:41.909+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SparkEnv: Registering MapOutputTracker
[2024-01-17T05:01:41.938+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-17T05:01:41.953+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-17T05:01:41.954+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-17T05:01:41.958+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-17T05:01:41.978+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1ed71c81-86a6-451b-8b3c-7edc7ba47c12
[2024-01-17T05:01:41.991+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-17T05:01:42.006+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-17T05:01:42.125+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-17T05:01:42.170+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-17T05:01:42.253+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://192.168.224.5:7077...
[2024-01-17T05:01:42.285+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO TransportClientFactory: Successfully created connection to /192.168.224.5:7077 after 19 ms (0 ms spent in bootstraps)
[2024-01-17T05:01:42.352+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240117050142-0092
[2024-01-17T05:01:42.353+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240117050142-0092/0 on worker-20240117040815-192.168.224.7-45465 (192.168.224.7:45465) with 2 core(s)
[2024-01-17T05:01:42.355+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20240117050142-0092/0 on hostPort 192.168.224.7:45465 with 2 core(s), 1024.0 MiB RAM
[2024-01-17T05:01:42.359+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36425.
[2024-01-17T05:01:42.359+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO NettyBlockTransferService: Server created on 614a419ad930:36425
[2024-01-17T05:01:42.361+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-17T05:01:42.368+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 614a419ad930, 36425, None)
[2024-01-17T05:01:42.371+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO BlockManagerMasterEndpoint: Registering block manager 614a419ad930:36425 with 434.4 MiB RAM, BlockManagerId(driver, 614a419ad930, 36425, None)
[2024-01-17T05:01:42.373+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 614a419ad930, 36425, None)
[2024-01-17T05:01:42.374+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 614a419ad930, 36425, None)
[2024-01-17T05:01:42.375+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240117050142-0092/0 is now RUNNING
[2024-01-17T05:01:42.547+0000] {spark_submit.py:571} INFO - 24/01/17 05:01:42 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-01-17T05:01:43.786+0000] {spark_submit.py:571} INFO - WARNING:root:kafka dataframe could not be created because: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[2024-01-17T05:01:43.789+0000] {spark_submit.py:571} INFO - ERROR:root:An error occurred in the Spark Streaming job: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.
[2024-01-17T05:01:44.071+0000] {spark_submit.py:571} INFO - Traceback (most recent call last):
[2024-01-17T05:01:44.071+0000] {spark_submit.py:571} INFO - File "/opt/***/dags/spark_stream.py", line 333, in <module>
[2024-01-17T05:01:44.071+0000] {spark_submit.py:571} INFO - main()
[2024-01-17T05:01:44.071+0000] {spark_submit.py:571} INFO - File "/opt/***/dags/spark_stream.py", line 330, in main
[2024-01-17T05:01:44.071+0000] {spark_submit.py:571} INFO - cassandra_session.shutdown()
[2024-01-17T05:01:44.071+0000] {spark_submit.py:571} INFO - UnboundLocalError: local variable 'cassandra_session' referenced before assignment
[2024-01-17T05:01:44.299+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/***/dags/spark_stream.py. Error code is: 1.
[2024-01-17T05:01:44.302+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=spark_stream_job, task_id=spark_submit_task, execution_date=20240117T015000, start_date=20240117T050138, end_date=20240117T050144
[2024-01-17T05:01:44.312+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 280 for task spark_submit_task (Cannot execute: spark-submit --master spark://192.168.224.5:7077 --name spark_submit_job --class org.apache.spark.deploy.PythonRunner --verbose --queue root.default --deploy-mode client /opt/***/dags/spark_stream.py. Error code is: 1.; 27605)
[2024-01-17T05:01:44.330+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-01-17T05:01:44.347+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
